{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Validation\n",
    "\n",
    "Now that you have a trained network, you can use it for making predictions. This is typically called **inference**, a term borrowed from statistics. However, neural networks have a tendency to perform *too well* on the training data and aren't able to generalize to data that hasn't been seen before. This is called **overfitting** and it impairs inference performance. To test for overfitting while training, we measure the performance on data not in the training set called the **validation** set. We avoid overfitting through regularization such as dropout while monitoring the validation performance during training. In this notebook, I'll show you how to do this in PyTorch. \n",
    "\n",
    "As usual, let's start by loading the dataset through torchvision. You'll learn more about torchvision and loading data in a later part. This time we'll be taking advantage of the test set which you can get by setting `train=False` here:\n",
    "\n",
    "```python\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "```\n",
    "\n",
    "The test set contains images just like the training set. Typically you'll see 10-20% of the original dataset held out for testing and validation with the rest being used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'll create a model like normal, using the same one from my solution for part 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of validation is to measure the model's performance on data that isn't part of the training set. Performance here is up to the developer to define though. Typically this is just accuracy, the percentage of classes the network predicted correctly. Other options are [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall#Definition_(classification_context)) and top-5 error rate. We'll focus on accuracy here. First I'll do a forward pass with one batch from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "model = Classifier()\n",
    "\n",
    "images, labels = next(iter(testloader))\n",
    "# Get the class probabilities\n",
    "ps = torch.exp(model(images))\n",
    "# Make sure the shape is appropriate, we should get 10 class probabilities for 64 examples\n",
    "print(ps.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the probabilities, we can get the most likely class using the `ps.topk` method. This returns the $k$ highest values. Since we just want the most likely class, we can use `ps.topk(1)`. This returns a tuple of the top-$k$ values and the top-$k$ indices. If the highest value is the fifth element, we'll get back 4 as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])\n"
     ]
    }
   ],
   "source": [
    "top_p, top_class = ps.topk(1, dim=1)\n",
    "# Look at the most likely classes for the first 10 examples\n",
    "print(top_class[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check if the predicted classes match the labels. This is simple to do by equating `top_class` and `labels`, but we have to be careful of the shapes. Here `top_class` is a 2D tensor with shape `(64, 1)` while `labels` is 1D with shape `(64)`. To get the equality to work out the way we want, `top_class` and `labels` must have the same shape.\n",
    "\n",
    "If we do\n",
    "\n",
    "```python\n",
    "equals = top_class == labels\n",
    "```\n",
    "\n",
    "`equals` will have shape `(64, 64)`, try it yourself. What it's doing is comparing the one element in each row of `top_class` with each element in `labels` which returns 64 True/False boolean values for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "equals = top_class == labels.view(*top_class.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to calculate the percentage of correct predictions. `equals` has binary values, either 0 or 1. This means that if we just sum up all the values and divide by the number of values, we get the percentage of correct predictions. This is the same operation as taking the mean, so we can get the accuracy with a call to `torch.mean`. If only it was that simple. If you try `torch.mean(equals)`, you'll get an error\n",
    "\n",
    "```\n",
    "RuntimeError: mean is not implemented for type torch.ByteTensor\n",
    "```\n",
    "\n",
    "This happens because `equals` has type `torch.ByteTensor` but `torch.mean` isn't implement for tensors with that type. So we'll need to convert `equals` to a float tensor. Note that when we take `torch.mean` it returns a scalar tensor, to get the actual value as a float we'll need to do `accuracy.item()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 10.9375%\n"
     ]
    }
   ],
   "source": [
    "accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "print(f'Accuracy: {accuracy.item()*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is untrained so it's making random guesses and we should see an accuracy around 10%. Now let's train our network and include our validation pass so we can measure how well the network is performing on the test set. Since we're not updating our parameters in the validation pass, we can speed up our code by turning off gradients using `torch.no_grad()`:\n",
    "\n",
    "```python\n",
    "# turn off gradients\n",
    "with torch.no_grad():\n",
    "    # validation pass here\n",
    "    for images, labels in testloader:\n",
    "        ...\n",
    "```\n",
    "\n",
    ">**Exercise:** Implement the validation loop below and print out the total accuracy after the loop. You can largely copy and paste the code from above, but I suggest typing it in because writing it out yourself is essential for building the skill. In general you'll always learn more by typing it rather than copy-pasting. You should be able to get an accuracy above 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Accuracy: 83.6484872611465% | Testing Loss: 0.45677462220191956 | Training Loss: 0.5114676925673414\n",
      "Epoch 2 Accuracy: 85.92754777070064% | Testing Loss: 0.3916541337966919 | Training Loss: 0.3902154937386513\n",
      "Epoch 3 Accuracy: 85.09156050955414% | Testing Loss: 0.3976951539516449 | Training Loss: 0.3502023282812348\n",
      "Epoch 4 Accuracy: 86.06687898089172% | Testing Loss: 0.40150171518325806 | Training Loss: 0.3355823245797076\n",
      "Epoch 5 Accuracy: 86.73367834394905% | Testing Loss: 0.3879126310348511 | Training Loss: 0.31569366270640514\n",
      "Epoch 6 Accuracy: 86.76353503184714% | Testing Loss: 0.3647049367427826 | Training Loss: 0.3001045930916185\n",
      "Epoch 7 Accuracy: 87.18152866242038% | Testing Loss: 0.3725961446762085 | Training Loss: 0.2900548457130313\n",
      "Epoch 8 Accuracy: 87.13176751592357% | Testing Loss: 0.3731285035610199 | Training Loss: 0.2849117983331177\n",
      "Epoch 9 Accuracy: 87.51990445859873% | Testing Loss: 0.368499219417572 | Training Loss: 0.27074826048857875\n",
      "Epoch 10 Accuracy: 87.609474522293% | Testing Loss: 0.3673646152019501 | Training Loss: 0.2698144905531267\n",
      "Epoch 11 Accuracy: 87.10191082802548% | Testing Loss: 0.37801676988601685 | Training Loss: 0.26125761925348084\n",
      "Epoch 12 Accuracy: 88.25636942675159% | Testing Loss: 0.35354098677635193 | Training Loss: 0.25181864013772276\n",
      "Epoch 13 Accuracy: 87.62937898089172% | Testing Loss: 0.37392497062683105 | Training Loss: 0.24750869653658317\n",
      "Epoch 14 Accuracy: 87.83837579617835% | Testing Loss: 0.38116440176963806 | Training Loss: 0.24057135653934245\n",
      "Epoch 15 Accuracy: 87.41042993630573% | Testing Loss: 0.3910645842552185 | Training Loss: 0.23512901900721384\n",
      "Epoch 16 Accuracy: 88.6046974522293% | Testing Loss: 0.36852791905403137 | Training Loss: 0.23162923011380726\n",
      "Epoch 17 Accuracy: 88.40565286624204% | Testing Loss: 0.3768780827522278 | Training Loss: 0.22636618897684221\n",
      "Epoch 18 Accuracy: 88.46536624203821% | Testing Loss: 0.37095341086387634 | Training Loss: 0.2225004148794644\n",
      "Epoch 19 Accuracy: 88.51512738853503% | Testing Loss: 0.37978842854499817 | Training Loss: 0.22028967917664474\n",
      "Epoch 20 Accuracy: 87.86823248407643% | Testing Loss: 0.407080739736557 | Training Loss: 0.2125988040826341\n",
      "Epoch 21 Accuracy: 88.51512738853503% | Testing Loss: 0.38970664143562317 | Training Loss: 0.20929322451718455\n",
      "Epoch 22 Accuracy: 88.46536624203821% | Testing Loss: 0.37982413172721863 | Training Loss: 0.20623764141536216\n",
      "Epoch 23 Accuracy: 88.47531847133759% | Testing Loss: 0.38531243801116943 | Training Loss: 0.20371791562720784\n",
      "Epoch 24 Accuracy: 88.0374203821656% | Testing Loss: 0.39216989278793335 | Training Loss: 0.19913674247608001\n",
      "Epoch 25 Accuracy: 88.26632165605095% | Testing Loss: 0.40461185574531555 | Training Loss: 0.19082373176921785\n",
      "Epoch 26 Accuracy: 87.51990445859873% | Testing Loss: 0.43564900755882263 | Training Loss: 0.19880398425227924\n",
      "Epoch 27 Accuracy: 87.97770700636943% | Testing Loss: 0.45375528931617737 | Training Loss: 0.18348751618250855\n",
      "Epoch 28 Accuracy: 88.38574840764332% | Testing Loss: 0.4176952540874481 | Training Loss: 0.19011347638820408\n",
      "Epoch 29 Accuracy: 88.05732484076432% | Testing Loss: 0.45768219232559204 | Training Loss: 0.17962469058488606\n",
      "Epoch 30 Accuracy: 88.29617834394905% | Testing Loss: 0.4302169382572174 | Training Loss: 0.18929057842346905\n"
     ]
    }
   ],
   "source": [
    "model = Classifier()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 30\n",
    "steps = 0\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    test_loss = 0 \n",
    "    accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for images,labels in testloader:\n",
    "            output = model(images)\n",
    "            probs = torch.exp(output)\n",
    "            test_loss += criterion(output,labels)\n",
    "            p,predictions = probs.topk(1,dim=1)\n",
    "            accuracy += (predictions==labels.view(*predictions.shape)).type(torch.FloatTensor).mean()\n",
    "    avg_test_loss = test_loss/len(testloader)\n",
    "    avg_train_loss = running_loss/len(trainloader)\n",
    "    print(f'Epoch {e+1} Accuracy: {accuracy.item()/len(testloader)*100}% | Testing Loss: {avg_test_loss} | Training Loss: {avg_train_loss}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "If we look at the training and validation losses as we train the network, we can see a phenomenon known as overfitting.\n",
    "\n",
    "<img src='assets/overfitting.png' width=450px>\n",
    "\n",
    "The network learns the training set better and better, resulting in lower training losses. However, it starts having problems generalizing to data outside the training set leading to the validation loss increasing. The ultimate goal of any deep learning model is to make predictions on new data, so we should strive to get the lowest validation loss possible. One option is to use the version of the model with the lowest validation loss, here the one around 8-10 training epochs. This strategy is called *early-stopping*. In practice, you'd save the model frequently as you're training then later choose the model with the lowest validation loss.\n",
    "\n",
    "The most common method to reduce overfitting (outside of early-stopping) is *dropout*, where we randomly drop input units. This forces the network to share information between weights, increasing it's ability to generalize to new data. Adding dropout in PyTorch is straightforward using the [`nn.Dropout`](https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout) module.\n",
    "\n",
    "```python\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        # Now with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        \n",
    "        # output so no dropout here\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x\n",
    "```\n",
    "\n",
    "During training we want to use dropout to prevent overfitting, but during inference we want to use the entire network. So, we need to turn off dropout during validation, testing, and whenever we're using the network to make predictions. To do this, you use `model.eval()`. This sets the model to evaluation mode where the dropout probability is 0. You can turn dropout back on by setting the model to train mode with `model.train()`. In general, the pattern for the validation loop will look like this, where you turn off gradients, set the model to evaluation mode, calculate the validation loss and metric, then set the model back to train mode.\n",
    "\n",
    "```python\n",
    "# turn off gradients\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # validation pass here\n",
    "    for images, labels in testloader:\n",
    "        ...\n",
    "\n",
    "# set model back to train mode\n",
    "model.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Add dropout to your model and train it on Fashion-MNIST again. See if you can get a lower validation loss or higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Define your model with dropout added\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(784,392)\n",
    "        self.h2 = nn.Linear(392,196)\n",
    "        self.h3 = nn.Linear(196,98)\n",
    "        self.o = nn.Linear(98,10)\n",
    "        # Dropout will randomly \"turn off\" nodes within each training pass to prevent overfitting\n",
    "        self.dropout = nn.Dropout(p=.2)\n",
    "    def forward(self,x):\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = self.dropout(F.relu(self.h1(x)))\n",
    "        x = self.dropout(F.relu(self.h2(x)))\n",
    "        x = self.dropout(F.relu(self.h3(x)))\n",
    "        x = F.log_softmax(self.o(x),dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "Training Accuracy: 0.8273920575692963\n",
      "Training Loss: 0.505073070526123\n",
      "Testing Accuracy: 0.8462380573248408\n",
      "Testing Loss: 0.45117565989494324\n",
      "Epoch 10\n",
      "Training Accuracy: 0.8346381929637526\n",
      "Training Loss: 0.49207645654678345\n",
      "Testing Accuracy: 0.839171974522293\n",
      "Testing Loss: 0.48548197746276855\n",
      "Epoch 15\n",
      "Training Accuracy: 0.8405850213219617\n",
      "Training Loss: 0.47611817717552185\n",
      "Testing Accuracy: 0.8357882165605095\n",
      "Testing Loss: 0.4619292616844177\n",
      "Epoch 20\n",
      "Training Accuracy: 0.8451159381663113\n",
      "Training Loss: 0.4613760709762573\n",
      "Testing Accuracy: 0.8505175159235668\n",
      "Testing Loss: 0.43372222781181335\n",
      "Epoch 25\n",
      "Training Accuracy: 0.849563566098081\n",
      "Training Loss: 0.45554736256599426\n",
      "Testing Accuracy: 0.8511146496815286\n",
      "Testing Loss: 0.45399966835975647\n",
      "Epoch 30\n",
      "Training Accuracy: 0.8445162579957356\n",
      "Training Loss: 0.4707377552986145\n",
      "Testing Accuracy: 0.8561902866242038\n",
      "Testing Loss: 0.44689610600471497\n"
     ]
    }
   ],
   "source": [
    "## TODO: Train your model with dropout, and monitor the training progress with the validation loss and accuracy\n",
    "epochs = 30\n",
    "model = Network()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.005)\n",
    "training_losses,testing_losses,training_accuracies,testing_accuracies = [],[],[],[]\n",
    "\n",
    "for e in range(epochs):\n",
    "    training_loss = 0\n",
    "    training_accuracy = 0\n",
    "    testing_loss = 0\n",
    "    testing_accuracy = 0\n",
    "    # Training passes\n",
    "    for images,labels in trainloader:\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Pass batch through model\n",
    "        log_ps = model(images)\n",
    "        # Calculate loss on model\n",
    "        loss = criterion(log_ps,labels)\n",
    "        training_loss += loss\n",
    "        # Calculate gradients, then update weighta\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Get probabilities & Class\n",
    "        probs = torch.exp(log_ps)\n",
    "        top_p, top_class = probs.topk(1,dim=1)\n",
    "        # Track no. correct\n",
    "        matches = (top_class == labels.view(*top_class.shape)).type(torch.FloatTensor)\n",
    "        accuracy = matches.mean().item()\n",
    "        training_accuracy += accuracy\n",
    "    \n",
    "    # Testing passes\n",
    "    with torch.no_grad():\n",
    "        # Turning off dropout by activating evaluation mode\n",
    "        model.eval()\n",
    "        for images, labels in testloader:\n",
    "            log_ps = model(images)\n",
    "            loss = criterion(log_ps,labels)\n",
    "            testing_loss += loss\n",
    "            ps = torch.exp(log_ps)\n",
    "            top_ps,top_class = ps.topk(1,dim=1)\n",
    "            matches = (top_class == labels.view(*top_class.shape)).type(torch.FloatTensor)\n",
    "            accuracy = matches.mean().item()\n",
    "            testing_accuracy += accuracy\n",
    "    # Turning dropout back on\n",
    "    model.train()\n",
    "    # Appending results for plotting\n",
    "    training_losses.append((training_loss/len(trainloader)).item())\n",
    "    training_accuracies.append(training_accuracy/len(trainloader))\n",
    "    testing_losses.append((testing_loss/len(testloader)).item())\n",
    "    testing_accuracies.append(testing_accuracy/len(testloader))\n",
    "    \n",
    "    # Printing every 5 epoch results\n",
    "    if (e+1)%5==0: \n",
    "        print(f'Epoch {e+1}')\n",
    "        print(f'Training Accuracy: {training_accuracy/len(trainloader)}')\n",
    "        print(f'Training Loss: {training_loss/len(trainloader)}')\n",
    "        print(f'Testing Accuracy: {testing_accuracy/len(testloader)}')\n",
    "        print(f'Testing Loss: {testing_loss/len(testloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x134afe3c8>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(training_losses,label='Training Loss')\n",
    "plt.plot(\n",
    "    testing_losses,label='Validation Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Now that the model is trained, we can use it for inference. We've done this before, but now we need to remember to set the model in inference mode with `model.eval()`. You'll also want to turn off autograd with the `torch.no_grad()` context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import helper module (should be in the repo)\n",
    "import helper\n",
    "\n",
    "# Test out your network!\n",
    "\n",
    "model.eval()\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "img = images[0]\n",
    "# Convert 2D image to 1D vector\n",
    "img = img.view(1, 784)\n",
    "\n",
    "# Calculate the class probabilities (softmax) for img\n",
    "with torch.no_grad():\n",
    "    output = model.forward(img)\n",
    "\n",
    "ps = torch.exp(output)\n",
    "\n",
    "# Plot the image and probabilities\n",
    "helper.view_classify(img.view(1, 28, 28), ps, version='Fashion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving our Model using torch.save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (h1): Linear(in_features=784, out_features=392, bias=True)\n",
       "  (h2): Linear(in_features=392, out_features=196, bias=True)\n",
       "  (h3): Linear(in_features=196, out_features=98, bias=True)\n",
       "  (o): Linear(in_features=98, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.2)\n",
       ")"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('h1.weight',\n",
       "              tensor([[ 0.6635,  0.6317,  0.6783,  ...,  0.0347,  0.4029,  0.6043],\n",
       "                      [ 0.6620,  0.6720,  0.7170,  ...,  0.7780,  0.6932,  0.6519],\n",
       "                      [ 0.2829,  0.2228,  0.2746,  ...,  0.2472,  0.3564,  0.2465],\n",
       "                      ...,\n",
       "                      [ 0.1973,  0.2188,  0.2075,  ..., -0.2337, -0.0223,  0.1776],\n",
       "                      [ 0.1578,  0.1640,  0.1244,  ...,  0.1436,  0.1222,  0.1421],\n",
       "                      [ 0.0838,  0.0753,  0.0645,  ...,  0.0424,  0.0509,  0.0721]])),\n",
       "             ('h1.bias',\n",
       "              tensor([-0.6652, -0.6343, -0.2729, -0.0999, -0.3086, -0.2899, -0.1437, -0.0717,\n",
       "                      -0.7657, -0.3900, -0.0302, -0.4355, -0.2352, -0.4811, -0.1490, -0.0904,\n",
       "                      -0.2654, -0.0993, -0.5458, -0.0679, -0.2055, -0.1234, -0.0891, -0.1885,\n",
       "                      -0.1709,  0.3557, -0.1333, -0.2390, -0.3208, -0.2140, -0.3272, -0.6511,\n",
       "                      -0.6942, -0.0365, -0.5369, -0.4165, -0.1422, -0.0607, -0.2982, -0.3450,\n",
       "                      -0.1518, -0.0486, -0.1016, -0.2103, -0.1785, -0.0715, -0.0710, -0.3402,\n",
       "                      -0.0289, -0.2843, -0.0142, -0.7158, -0.6437, -0.2483, -0.2013, -0.2433,\n",
       "                      -0.0262, -0.4421, -0.2980, -0.4082, -0.0726, -0.1578, -0.3099, -0.0649,\n",
       "                      -0.1839,  0.0849, -0.1520, -0.3759, -0.2398, -0.7390, -0.4593, -0.2062,\n",
       "                      -0.0609, -0.3272, -0.0539, -0.4661, -0.2356, -0.5939, -0.3882, -0.0620,\n",
       "                      -0.1108, -0.4723, -0.2968, -0.2679, -0.1024,  0.0081, -0.4644, -0.2239,\n",
       "                      -0.1163, -0.3931, -0.2697, -0.2283, -0.0491, -0.3264, -0.2977, -0.0609,\n",
       "                      -0.4770, -0.2940, -0.1259, -0.5155, -0.4434, -0.4120, -0.3915, -0.5054,\n",
       "                      -0.0353, -0.3582, -0.0368, -0.1525, -0.4445, -0.0615, -0.0203, -0.1636,\n",
       "                      -0.3175, -0.0716, -0.1618, -0.1284, -0.3572, -0.2520, -0.3190, -0.2942,\n",
       "                      -0.2514, -0.2606, -0.1032, -0.4153, -0.0398, -0.1287, -0.7525, -0.0329,\n",
       "                      -0.4573, -0.4696, -0.2153, -0.5301, -0.2339, -0.3366, -0.3475, -0.2809,\n",
       "                      -0.3083, -0.2132, -0.2176, -0.0314, -0.2534, -0.1936, -0.2967, -0.1427,\n",
       "                      -0.1879, -0.2152, -0.5503, -0.4905, -0.0588, -0.9600, -0.1334, -0.4662,\n",
       "                      -0.0784, -0.2347, -0.0624, -0.3125, -0.4618, -0.2208, -0.2367, -0.1110,\n",
       "                      -0.4293, -0.3638, -0.4762, -0.0390, -0.5621, -0.1030, -0.1350, -0.0641,\n",
       "                      -0.0497, -0.3087, -0.0543, -0.2590, -0.0433, -0.4001, -0.5347, -0.0935,\n",
       "                      -0.3482, -0.1368, -0.1378, -0.2452, -0.0220, -0.3026, -0.0198, -0.0809,\n",
       "                      -0.3547, -0.0747, -0.0440, -0.3120, -0.5478, -0.5646, -0.0545, -0.3461,\n",
       "                      -0.2895, -0.0457, -0.0560, -0.4933, -0.3211, -0.5336, -0.5070, -0.3196,\n",
       "                      -0.1228, -0.5380, -0.0823, -0.1181, -0.1611, -0.2395, -0.2008, -0.3780,\n",
       "                      -0.1372, -0.1253, -0.2524, -0.2358, -0.1016, -0.1355, -0.0953, -0.0606,\n",
       "                      -0.0657, -0.2033, -0.0691, -0.2129, -0.3467, -0.4505, -0.3327, -0.1521,\n",
       "                      -0.3058, -0.0696, -0.1257, -0.0596, -0.0504, -0.6527, -0.0560, -0.0825,\n",
       "                      -0.4122, -0.0932, -0.1762, -0.0349, -0.3610, -0.5102, -0.1689, -0.1601,\n",
       "                      -0.6920, -0.0303, -0.2095, -0.8167, -0.0353, -0.1433, -0.0739, -0.2536,\n",
       "                      -0.5781, -0.4523, -0.0737, -0.0659, -0.5063, -0.3581, -0.0707, -0.0379,\n",
       "                      -0.3493, -0.1870, -0.2328, -0.0393, -0.1546, -0.4214, -0.1542, -0.1678,\n",
       "                      -0.6983, -0.5797,  0.2270, -0.3274, -0.3906, -0.2832, -0.0707, -0.3137,\n",
       "                      -0.4720, -0.1595, -0.4646, -0.1647, -0.6295, -0.2926, -0.4405, -0.4155,\n",
       "                      -0.1050, -0.0125, -0.3055, -0.0325, -0.2390, -0.2489, -0.3486, -0.0882,\n",
       "                      -0.2954, -0.0709, -0.4963, -0.1234, -0.4749, -0.2405, -0.0898, -0.2974,\n",
       "                      -0.0995, -0.1967, -0.0083, -0.6662, -0.1281, -0.1596, -0.1818, -0.4328,\n",
       "                      -0.3845, -0.1117, -0.0214, -0.4282, -0.7230, -0.4727, -0.1357, -0.3084,\n",
       "                      -0.1518, -0.1055, -0.5030, -0.0963, -0.5729, -0.1482, -0.5832,  0.7324,\n",
       "                      -0.1581, -0.0736, -0.2557, -0.2326, -0.1663, -0.1622, -0.1095, -0.3878,\n",
       "                      -0.2923, -0.6673, -0.6394, -0.3599, -0.1837, -0.2737, -0.6059, -0.1647,\n",
       "                      -0.2773, -0.0668, -0.0739, -0.4045, -0.3202, -0.2920, -0.2786, -0.2590,\n",
       "                      -0.3034, -0.4192, -0.1504, -0.6600, -0.0866, -0.2178, -0.0438, -0.3919,\n",
       "                      -0.1052, -0.1762, -0.0518, -0.3757, -0.1423, -0.0486, -0.7486, -0.0547,\n",
       "                      -0.0796, -0.1979, -0.3888, -0.3032, -0.1514, -0.1781, -0.0064, -0.7095,\n",
       "                      -0.3626, -0.0733, -0.2351, -0.4623, -0.2885, -0.4197, -0.4444, -0.0898,\n",
       "                      -0.3687, -0.2194, -0.0373, -0.0729, -0.3579, -0.3036, -0.1971, -0.0755,\n",
       "                      -0.2651, -0.0710, -0.0733, -0.2321, -0.0422, -0.2395, -0.1271, -0.0765])),\n",
       "             ('h2.weight',\n",
       "              tensor([[ 0.5665, -0.6494,  0.1304,  ...,  0.1758,  0.0131, -0.0154],\n",
       "                      [-0.5531, -0.7338, -0.2200,  ..., -0.0295,  0.0367, -0.0637],\n",
       "                      [ 0.0379, -0.1093, -0.9919,  ..., -0.4423,  0.2343, -0.0947],\n",
       "                      ...,\n",
       "                      [ 0.2178, -0.3875, -0.1372,  ..., -1.0426, -0.2529,  0.0154],\n",
       "                      [ 0.6982, -0.7661, -0.4249,  ..., -0.4155, -0.0473,  0.0195],\n",
       "                      [-0.3789, -0.3805, -0.3798,  ..., -0.6692,  0.1531,  0.0258]])),\n",
       "             ('h2.bias',\n",
       "              tensor([-1.5435e+00, -1.3552e+00,  1.0399e-01, -2.3664e+00, -1.7831e+00,\n",
       "                      -3.1740e+00, -1.1657e+00, -6.3832e-02, -6.4369e-01,  2.3912e-02,\n",
       "                      -4.3545e-01, -2.7548e+00, -2.4808e+00,  1.5373e-01, -1.5721e+00,\n",
       "                       1.5400e-01, -1.3202e-01,  8.0410e-01, -3.3362e-02, -2.7887e+00,\n",
       "                      -5.6820e-01,  4.6902e-01, -3.2990e+00, -2.1378e+00, -1.6258e+00,\n",
       "                      -2.5671e+00, -1.8737e+00, -3.2015e+00, -1.2941e+00, -2.1534e+00,\n",
       "                      -2.3061e+00, -2.6533e+00, -1.8039e-01,  2.2461e-01,  3.8681e-01,\n",
       "                       2.1081e-01, -3.3864e+00, -3.5205e+00,  9.4573e-01,  8.2843e-01,\n",
       "                      -2.7083e+00, -1.9377e+00,  4.3036e-01, -2.0884e+00, -8.3240e-01,\n",
       "                      -2.1255e+00, -1.6126e+00, -3.9483e+00, -3.6561e+00, -4.2140e-01,\n",
       "                      -1.7253e+00, -2.6481e+00, -1.6821e+00, -3.2526e+00, -3.1900e+00,\n",
       "                      -2.6005e-02, -2.4487e+00, -2.9820e+00, -1.4561e+00, -1.4447e-01,\n",
       "                      -2.0812e+00, -6.8057e-01, -2.1239e-01, -2.6893e+00, -1.0484e+00,\n",
       "                      -1.9314e+00, -2.4547e+00, -2.3055e+00, -1.8538e+00, -2.8168e+00,\n",
       "                      -7.6832e-01, -3.5595e+00, -2.0689e+00, -1.6527e-01, -2.2698e+00,\n",
       "                       1.2675e-01, -3.3544e+00, -1.5878e+00, -8.9925e-01, -5.6601e-01,\n",
       "                      -3.6768e+00, -1.8327e+00, -2.3505e+00, -1.6881e-01, -3.2961e+00,\n",
       "                      -2.0170e+00, -2.5633e+00, -1.9024e+00, -2.9769e+00, -1.2003e-03,\n",
       "                      -1.1279e+00, -3.5646e+00, -3.8145e+00, -2.0198e+00, -1.3489e+00,\n",
       "                      -2.9413e+00, -1.1764e+00, -3.3401e+00, -1.6785e+00, -1.4654e+00,\n",
       "                      -1.7755e-01, -1.2151e+00, -3.7652e+00, -2.5707e+00, -5.5560e-02,\n",
       "                      -2.3769e+00,  1.1667e-01,  5.5293e-01, -2.2245e+00, -1.1566e+00,\n",
       "                       1.6741e-01,  6.7772e-01, -2.5196e+00, -7.5379e-01, -3.0700e+00,\n",
       "                      -2.5824e+00, -1.9775e+00, -1.4699e+00, -1.4700e-01, -2.1954e+00,\n",
       "                      -3.2470e+00, -5.4336e-02, -8.1316e-01,  2.9839e-01, -2.1294e+00,\n",
       "                      -2.8617e+00, -1.2877e+00, -1.5665e+00, -2.7253e+00, -1.9623e+00,\n",
       "                      -2.8144e+00, -1.8026e+00, -2.9227e+00,  3.9764e-01,  1.8676e-01,\n",
       "                       5.1454e-01, -3.5664e+00, -3.4575e+00, -1.6658e+00,  7.3666e-01,\n",
       "                      -2.0308e+00, -5.4133e-02, -1.6065e+00,  8.6995e-01, -2.1704e+00,\n",
       "                      -3.4339e+00, -3.0888e+00, -3.4942e-03, -3.3668e+00, -2.2128e+00,\n",
       "                      -6.7904e-01, -3.2639e+00, -5.3756e-01, -3.6950e+00, -9.5049e-01,\n",
       "                       6.7204e-01, -1.1001e+00, -2.3119e+00, -2.0995e+00, -2.1756e+00,\n",
       "                      -3.3381e+00, -1.6310e+00, -4.0523e+00, -1.4200e+00, -1.0008e+00,\n",
       "                      -2.1644e-02, -4.6473e-01, -1.8164e-01, -1.8773e+00,  5.2176e-01,\n",
       "                      -1.1146e+00, -2.1544e+00, -1.6444e+00, -9.5193e-02, -2.3508e+00,\n",
       "                      -3.3461e-01, -6.1814e-01, -3.0467e+00, -1.9458e+00,  2.2709e-01,\n",
       "                       4.1700e-01, -3.5464e+00, -8.4255e-01, -7.9023e-02, -2.7858e+00,\n",
       "                       1.9467e-01,  3.8069e-01, -2.4473e-02,  8.0008e-01, -2.3495e+00,\n",
       "                      -1.4914e+00, -3.0675e+00, -3.0546e+00, -2.1270e+00, -2.8242e+00,\n",
       "                      -9.3915e-01])),\n",
       "             ('h3.weight',\n",
       "              tensor([[-0.4110, -0.2564, -0.5559,  ..., -0.1928,  0.0416, -0.5057],\n",
       "                      [-0.2140, -0.0286, -0.7471,  ..., -0.2477, -0.1089,  0.1631],\n",
       "                      [-0.1266, -0.2885, -0.1877,  ...,  0.3731,  0.1129, -0.5543],\n",
       "                      ...,\n",
       "                      [-0.4008, -0.1925, -1.0479,  ..., -0.3716, -0.6747,  0.0055],\n",
       "                      [-0.1909, -0.2211, -1.0243,  ..., -0.3000, -0.1023, -0.2158],\n",
       "                      [-0.7579, -0.4841, -0.0871,  ..., -0.1872, -0.3491,  0.4415]])),\n",
       "             ('h3.bias',\n",
       "              tensor([-1.9270,  1.3259, -0.6944, -3.6528, -3.5744,  1.6230, -1.8968,  0.3914,\n",
       "                       1.0385,  2.4112,  0.9177, -3.3913,  1.3021,  0.9741,  0.8355,  1.2464,\n",
       "                      -2.6403, -2.7420, -0.2084, -3.2952, -0.8027, -2.9395, -2.0222,  0.6824,\n",
       "                       0.5230,  1.1106, -1.6171, -3.4816,  2.4350,  1.7452,  0.9124, -1.9960,\n",
       "                       0.4311, -2.3804, -0.1778, -0.8650, -2.2876,  0.0601,  1.0442, -1.3451,\n",
       "                      -2.4511, -2.4920, -3.3654, -2.8596, -2.9755,  0.1576,  0.8388, -2.5850,\n",
       "                      -1.7503, -3.2069, -2.2482,  1.6152, -2.0272,  1.0155, -1.8515,  1.6564,\n",
       "                       1.8630,  1.0332, -3.5183,  1.5280,  0.9362,  1.0355,  0.7870, -2.9110,\n",
       "                       1.1668, -3.0438, -2.4818, -3.2503, -2.2327, -2.6998, -2.4905,  0.3789,\n",
       "                       0.9276,  0.6142, -1.7600, -1.0963, -3.0667,  1.2790,  0.3929,  1.0461,\n",
       "                       0.7603, -1.5717, -2.0507, -3.1856, -3.3989,  1.2182, -2.1285, -3.2191,\n",
       "                      -2.9655,  1.9186, -3.2449, -2.5403,  1.2877,  0.8484, -2.8324,  0.9992,\n",
       "                      -1.7133, -3.2218])),\n",
       "             ('o.weight',\n",
       "              tensor([[-1.9234e-01, -2.0392e-02,  1.0481e-01, -8.4606e-01, -2.4985e-01,\n",
       "                       -3.0678e-04, -3.8632e-02,  5.8521e-02, -3.3488e-01, -1.2923e-01,\n",
       "                       -4.1323e-01, -5.3015e-01, -2.5373e-01, -3.8867e-01, -1.8995e-01,\n",
       "                       -6.4196e-02, -8.3333e-01, -4.0994e-01,  6.8739e-02, -1.7357e-01,\n",
       "                        5.8822e-02, -1.4631e+00, -2.7855e-01,  1.1102e-01,  1.0113e-01,\n",
       "                       -1.8852e-02, -3.9692e-01, -3.0763e-01, -2.0068e-01,  1.0765e-01,\n",
       "                       -5.5327e-02,  1.4322e-01, -5.9226e-02, -4.5592e-01,  1.0582e-02,\n",
       "                       -1.5212e-01, -7.6006e-01, -1.1964e-02, -3.9285e-02, -1.6659e-01,\n",
       "                       -9.0559e-02, -3.2806e-01, -4.1844e-01, -3.2178e-01, -5.2669e-01,\n",
       "                        4.1047e-02, -3.5199e-01, -7.1059e-01, -3.4225e-01, -1.1065e+00,\n",
       "                       -4.3778e-01,  1.0340e-01, -4.6064e-01,  1.6318e-04, -4.1228e-01,\n",
       "                        1.1867e-01, -6.0195e-02, -6.6756e-01, -1.0632e-01,  1.1791e-01,\n",
       "                        3.4648e-02, -3.2833e-01, -4.1097e-01, -4.4120e-01, -2.1580e-01,\n",
       "                       -8.0816e-01, -3.9806e-01, -3.6816e-01,  2.1238e-01, -3.9080e-01,\n",
       "                       -1.8546e-02, -2.1229e-01,  8.3097e-02,  6.0822e-02, -1.8401e-02,\n",
       "                       -1.2010e-01, -3.9016e-01,  1.6120e-01, -1.2258e-01, -1.3141e-01,\n",
       "                       -2.9459e-01, -9.4209e-02, -4.5429e-01, -1.9602e-01, -1.3236e-01,\n",
       "                       -3.6590e-01, -1.5397e-02, -1.8814e-01,  5.1120e-02, -1.7046e-01,\n",
       "                       -1.6578e-01, -4.2406e-01,  6.3101e-03, -2.2364e-01, -1.5229e-01,\n",
       "                       -5.0592e-02, -2.6943e-01, -1.7907e-01],\n",
       "                      [-2.0223e-02,  9.3189e-02, -1.2809e-02, -5.6003e-01, -2.7675e-01,\n",
       "                       -1.3911e-01, -4.6306e-01,  1.1617e-01, -3.3544e-01, -1.5588e-01,\n",
       "                       -3.7080e-01, -6.1663e-01, -1.3931e-01, -5.0355e-01, -2.0685e-01,\n",
       "                        7.5613e-02, -7.1284e-01, -6.1171e-01,  2.0607e-02, -7.3848e-01,\n",
       "                        3.6322e-01, -9.3607e-01, -4.7486e-01,  1.0096e-01,  4.0209e-02,\n",
       "                       -2.9974e-02, -3.3240e-01, -7.4555e-01, -1.8913e-02, -4.4221e-02,\n",
       "                        9.9278e-02, -4.2486e-01, -3.3746e-01,  6.1161e-02, -1.1168e-02,\n",
       "                       -2.0361e-02, -6.2058e-01, -6.7045e-02, -3.0847e-02, -4.7677e-01,\n",
       "                       -6.7087e-02, -1.1269e-01, -2.2388e-01, -2.9809e-01, -4.3837e-01,\n",
       "                       -1.1624e-01, -1.3669e-01, -1.8059e-01, -3.6600e-01, -6.0429e-01,\n",
       "                       -7.1404e-01,  8.7330e-02, -2.2463e-01,  2.2545e-01, -1.7390e-01,\n",
       "                        7.5932e-02,  8.6393e-02, -3.2723e-01, -5.1989e-02, -8.0292e-02,\n",
       "                       -1.7509e-01, -2.0744e-01, -8.7897e-02, -1.5312e-01, -2.5382e-01,\n",
       "                       -2.0326e-01, -1.0581e+00, -2.9085e-01, -2.8910e-01, -4.9349e-01,\n",
       "                       -1.7578e-01,  2.7743e-02,  4.7116e-02, -1.0047e-01, -3.1654e-01,\n",
       "                        1.2703e-01, -2.6230e-02, -4.3751e-02, -2.3436e-01, -1.1837e-01,\n",
       "                       -2.9399e-01,  5.8717e-02, -4.0129e-01, -2.8931e-01, -5.2082e-02,\n",
       "                       -3.8528e-01,  1.3005e-01, -1.8831e-01, -2.7081e-01,  3.4468e-02,\n",
       "                       -2.7875e-01, -3.6907e-01,  1.4438e-01, -9.6696e-02, -1.0317e-01,\n",
       "                        7.4305e-02, -1.0316e+00, -1.6450e-01],\n",
       "                      [-2.6956e-01, -1.5862e-02,  8.5707e-02, -5.7867e-01, -2.0022e-01,\n",
       "                        1.0339e-01,  5.3706e-02, -2.0057e-02, -4.0386e-01,  6.7846e-02,\n",
       "                       -6.3963e-01, -5.1776e-01,  6.9138e-02, -4.8486e-01,  1.7856e-03,\n",
       "                       -1.9906e-02, -5.1098e-01, -4.8878e-01,  9.7062e-03, -3.3626e-01,\n",
       "                       -2.4908e-02, -1.1058e+00, -2.3607e-01,  1.2351e-01,  8.6847e-02,\n",
       "                        1.6635e-03, -2.9353e-01, -3.4212e-01, -3.3395e-01,  3.7120e-02,\n",
       "                       -3.2757e-01, -2.5764e-01, -8.4581e-03, -5.9763e-01,  1.4922e-02,\n",
       "                       -4.4555e-01, -1.0269e+00, -2.0787e-01,  4.9417e-02, -1.6869e-01,\n",
       "                        7.7742e-02, -2.7115e-01, -2.2909e-01, -1.7801e-01, -3.5184e-02,\n",
       "                       -6.8503e-02, -6.5730e-02, -2.3191e-01, -2.5156e-01, -6.9111e-01,\n",
       "                       -5.1933e-01, -9.7853e-02, -4.1939e-01,  1.6304e-01, -2.8365e-01,\n",
       "                       -2.2582e-01,  7.0591e-02, -5.4872e-01, -6.2038e-01, -4.1959e-02,\n",
       "                       -7.4756e-02,  1.3981e-02, -4.5751e-01, -5.0494e-01, -2.3401e-01,\n",
       "                       -3.9140e-01, -5.1935e-01, -4.0881e-01, -1.4238e-01, -1.7415e-01,\n",
       "                       -1.0550e-01, -2.6113e-01, -1.5253e-01, -4.5861e-02, -1.5113e-01,\n",
       "                       -4.7604e-02, -2.0269e-01, -1.0011e-01,  1.0945e-03,  5.2888e-02,\n",
       "                       -5.3868e-01, -1.2321e-01, -1.2001e-01, -3.0595e-01, -2.1236e-01,\n",
       "                       -5.9749e-01, -8.2944e-02, -1.3409e-01, -3.1764e-01,  5.8371e-02,\n",
       "                       -4.8557e-01, -4.9142e-01, -8.2778e-02,  7.1244e-02,  9.5528e-03,\n",
       "                       -1.1882e-01, -2.2129e-01, -5.1356e-01],\n",
       "                      [ 2.7610e-01,  8.7969e-02,  2.3888e-02, -8.0481e-01, -1.7688e-01,\n",
       "                       -1.0676e-01,  1.6712e-01,  3.9101e-02, -2.2290e-01, -5.8436e-02,\n",
       "                       -2.5520e-01, -4.9240e-01, -1.8983e-01, -4.7088e-01, -4.0033e-01,\n",
       "                        5.2619e-03, -3.4719e-01, -3.0246e-01, -3.7529e-02, -9.3895e-01,\n",
       "                        2.1785e-01, -8.4805e-01,  2.1099e-01, -9.7565e-02, -3.7044e-02,\n",
       "                        1.1098e-01, -2.7207e-01, -3.6735e-01, -2.9909e-01, -7.0785e-02,\n",
       "                       -7.9227e-02, -1.3951e-01, -3.8361e-01, -2.0609e-01, -3.8996e-01,\n",
       "                       -1.1692e-01, -2.5025e-01, -2.2281e-01, -6.2944e-02, -8.0971e-01,\n",
       "                       -1.0824e-01, -5.3167e-01, -4.5228e-01, -1.9230e-01, -2.9704e-01,\n",
       "                       -7.4755e-02,  1.9265e-02, -2.0599e-01, -5.1775e-01, -6.4292e-01,\n",
       "                       -2.9001e-01,  1.5398e-01, -3.5206e-01,  4.4484e-02, -2.4055e-01,\n",
       "                       -3.7969e-03, -1.0280e-01, -5.1669e-01, -1.4132e-01, -9.9336e-02,\n",
       "                       -1.2007e-01, -2.1444e-01, -2.8022e-01, -8.6720e-02, -3.6152e-01,\n",
       "                       -7.7595e-01, -6.3854e-01, -4.3540e-01, -3.7993e-01, -4.6383e-01,\n",
       "                       -2.8100e-02, -4.5796e-02,  1.0039e-01, -9.1461e-02, -4.4954e-02,\n",
       "                       -4.4241e-02,  7.3695e-02,  6.3592e-03, -1.3515e-01, -3.3294e-01,\n",
       "                       -3.0844e-01, -2.1980e-02, -1.6039e-01, -3.3565e-01, -2.1751e-01,\n",
       "                       -3.8988e-01,  1.2752e-01, -2.1912e-01, -5.2645e-01, -3.6088e-02,\n",
       "                       -1.6002e-01, -4.8878e-01,  1.4171e-01,  9.4838e-02, -3.9430e-01,\n",
       "                        1.0585e-01, -1.2203e+00, -3.6481e-01],\n",
       "                      [ 2.5529e-02,  8.1359e-03,  4.7088e-02, -6.6894e-01, -2.0471e-01,\n",
       "                       -1.7118e-02,  9.4947e-02, -1.0728e-02, -3.5267e-01, -4.6667e-02,\n",
       "                       -4.8586e-01, -5.5204e-01,  1.2777e-01, -4.6390e-01,  2.5457e-02,\n",
       "                        4.9312e-02, -3.6495e-01, -2.9378e-01, -1.2901e-01, -3.4605e-01,\n",
       "                       -9.6697e-02, -6.6070e-01, -6.9578e-02,  1.5088e-02,  8.0132e-02,\n",
       "                        1.0356e-01, -2.3029e-01, -4.2995e-01, -2.8614e-01, -1.6389e-01,\n",
       "                       -1.1691e-01, -1.9463e-01,  1.0515e-01, -1.8765e-01,  1.0264e-01,\n",
       "                       -3.6632e-01, -5.2143e-01, -1.7667e-01,  9.3417e-02, -1.7389e-01,\n",
       "                       -1.3172e-01, -7.4512e-01, -7.0437e-01, -1.9446e-01, -4.8076e-01,\n",
       "                       -3.4790e-02,  8.4649e-02, -9.7472e-02, -3.4022e-01, -1.1572e+00,\n",
       "                       -5.6891e-01, -1.3316e-01, -2.7414e-01, -3.3570e-02, -5.1385e-01,\n",
       "                       -2.2037e-01,  3.9476e-02, -4.1176e-01,  2.4985e-02, -4.9499e-02,\n",
       "                       -2.7956e-01,  6.4092e-02, -3.4626e-01, -9.6991e-01, -2.4206e-01,\n",
       "                       -4.8220e-01, -9.1850e-01, -4.5574e-01, -2.5293e-01, -2.9793e-01,\n",
       "                        2.0000e-02,  5.5788e-02, -1.8126e-01,  3.1386e-02, -3.0474e-01,\n",
       "                        2.9063e-02, -1.1559e-02, -7.7304e-02,  6.8082e-02,  3.8522e-02,\n",
       "                       -4.8098e-01, -1.3494e-01, -1.9621e-01, -2.4482e-01, -1.2186e-01,\n",
       "                       -3.2855e-01, -7.5455e-01, -2.6855e-01, -3.0145e-01,  4.3826e-02,\n",
       "                       -1.7124e-01, -4.0642e-01,  5.6571e-02,  1.3135e-01, -7.6231e-02,\n",
       "                       -1.0629e-01, -3.1276e-01, -4.4139e-01],\n",
       "                      [-9.1310e-02, -1.9628e-01, -1.3181e-02, -2.0604e-01, -7.3838e-01,\n",
       "                       -1.6279e-01, -2.0942e-01,  1.5013e-01,  1.2487e-01,  1.3629e-01,\n",
       "                       -4.0493e-03, -6.4569e-01, -1.1655e-01,  7.4777e-02, -6.0700e-01,\n",
       "                       -3.6555e-01, -2.5439e-01, -4.5057e-02, -2.9257e-03, -7.7618e-02,\n",
       "                       -2.1926e-01, -7.6314e-01, -4.6268e-01, -1.7963e-01, -3.6202e-04,\n",
       "                       -3.0043e-01, -1.6356e-01, -3.1252e-01,  1.6627e-01, -2.1943e-01,\n",
       "                        1.0981e-01, -2.1019e-01, -1.9202e-01,  2.9213e-02,  1.3316e-02,\n",
       "                        5.0107e-02, -6.8600e-01,  2.0551e-01, -2.5352e-01,  6.2030e-02,\n",
       "                       -5.6650e-01, -6.4165e-01, -6.8429e-02, -6.8076e-02,  1.3316e-02,\n",
       "                        7.3908e-02, -5.4190e-01, -1.7307e-01, -6.0448e-01, -5.8795e-01,\n",
       "                       -1.5463e-01, -1.6271e-01, -4.8793e-01, -6.8105e-02, -1.2771e-01,\n",
       "                       -3.4476e-01, -2.9775e-01,  7.2811e-02, -2.2257e-01,  8.2680e-02,\n",
       "                        1.6000e-01, -3.7352e-01,  9.1123e-02, -3.2200e-01,  1.2165e-01,\n",
       "                       -6.7461e-02, -5.5125e-01, -3.2387e-01, -1.1077e+00,  7.8804e-02,\n",
       "                       -4.5879e-01,  1.0958e-02, -1.8659e-01, -3.8021e-01, -4.5693e-01,\n",
       "                       -1.4894e-01, -5.9550e-01, -1.3716e-01, -2.3569e-01, -4.8260e-01,\n",
       "                        4.4806e-02, -1.5770e-01, -2.2595e-01, -3.1467e-01, -1.8914e-01,\n",
       "                        5.1954e-02, -1.6116e-01, -9.8763e-01, -8.7075e-02, -2.1828e-01,\n",
       "                       -9.6606e-01, -3.3996e-01, -2.4010e-01, -4.1518e-01, -1.2740e-01,\n",
       "                       -7.0925e-02, -1.1647e-01, -7.6939e-01],\n",
       "                      [-1.2326e-01, -9.1631e-02,  1.6377e-01, -3.9861e-01, -3.3557e-01,\n",
       "                        1.0156e-01,  5.5104e-02,  4.5794e-02, -3.4659e-01,  3.4519e-02,\n",
       "                       -3.9397e-01, -4.2861e-01,  1.0816e-01, -4.5315e-01, -7.9866e-02,\n",
       "                       -5.1686e-02, -5.0692e-01, -4.1810e-01, -5.1410e-02, -8.9267e-02,\n",
       "                        3.2152e-02, -9.6017e-01, -3.7393e-01,  9.7063e-02,  9.7520e-02,\n",
       "                        3.0622e-02, -3.6101e-01, -4.8962e-01, -2.6188e-01,  1.0960e-01,\n",
       "                       -1.3460e-01, -1.8141e-01,  3.4836e-02, -2.1468e-01,  2.8970e-03,\n",
       "                       -1.4874e-01, -7.3836e-01, -5.9450e-02,  1.8391e-01, -2.1586e-01,\n",
       "                       -1.4733e-01, -2.5778e-01, -3.0486e-01, -1.8602e-01, -7.8020e-01,\n",
       "                        2.0473e-02,  4.3595e-02, -2.0660e-01, -3.6191e-01, -7.7239e-01,\n",
       "                       -4.4940e-01,  9.2762e-02, -1.7107e-01, -3.6253e-02, -4.3444e-01,\n",
       "                        4.1781e-02,  3.7009e-02, -7.6080e-01, -1.0528e-01,  1.0524e-01,\n",
       "                       -2.6246e-01, -9.4286e-02, -2.5098e-01, -4.4294e-01, -1.9654e-01,\n",
       "                       -2.8053e-01, -5.7387e-01, -2.7590e-01, -2.3916e-01, -4.0527e-01,\n",
       "                       -5.3715e-02, -1.3205e-01,  5.4399e-02,  1.3792e-01, -3.3712e-02,\n",
       "                        1.0187e-01, -1.5057e-01,  1.1309e-01,  1.4072e-01, -2.1243e-02,\n",
       "                       -3.3966e-01, -3.1909e-02, -9.9037e-02, -1.4681e-01, -1.3715e-01,\n",
       "                       -5.7588e-01,  1.3991e-01, -1.4633e-01, -4.5083e-01, -1.9651e-02,\n",
       "                       -2.4675e-01, -3.4044e-01, -6.3513e-02,  6.3862e-02, -1.5441e-01,\n",
       "                       -3.3501e-02, -2.4070e-01, -1.9097e-01],\n",
       "                      [ 4.4322e-02, -2.6251e-01,  1.1973e-01, -2.5137e-01, -6.7071e-01,\n",
       "                       -3.6944e-01,  1.3779e-01, -2.7227e-02,  6.9558e-02,  6.1839e-02,\n",
       "                        1.2696e-01, -7.1387e-01, -6.1842e-01,  8.1697e-02, -5.4608e-01,\n",
       "                       -2.0451e-01, -3.1318e-01, -3.4733e-01, -1.2807e-01, -2.6323e-01,\n",
       "                       -3.6847e-01, -6.9091e-01, -3.6975e-02, -8.4762e-01,  5.1659e-02,\n",
       "                       -4.1577e-01,  5.4473e-02, -1.5061e-01,  9.8946e-02, -7.3106e-01,\n",
       "                        1.6465e-01, -3.4602e-01, -3.6834e-01,  4.6344e-02,  1.3479e-01,\n",
       "                        9.3781e-02, -4.2380e-01,  7.2575e-02, -1.7805e-01, -6.3213e-02,\n",
       "                       -8.9896e-01, -3.2754e-01, -6.3098e-02, -1.0018e-01, -4.2445e-02,\n",
       "                        4.8019e-03, -7.5056e-01,  6.0299e-02, -6.4407e-01, -4.1281e-01,\n",
       "                       -1.7766e-01, -4.0416e-01, -8.0473e-01,  8.1331e-02, -3.6939e-01,\n",
       "                       -3.8163e-01, -2.1953e-01,  7.7147e-02,  5.7171e-02, -1.0949e-02,\n",
       "                        8.2327e-02, -2.6818e-01,  1.1218e-01, -1.4522e-01,  5.1491e-02,\n",
       "                       -9.0205e-02, -7.4493e-01, -1.9258e-01, -1.3984e-01, -3.1711e-01,\n",
       "                       -5.0031e-01, -2.1867e-02, -4.8001e-01, -6.3641e-01, -6.5373e-01,\n",
       "                        4.8865e-02, -7.5491e-02, -3.0565e-01, -2.7780e-01, -5.4826e-01,\n",
       "                        1.4033e-01, -1.1707e+00, -1.3613e-01, -2.3528e-01, -9.8549e-01,\n",
       "                        1.3835e-01, -9.3348e-02, -6.9579e-01, -1.2322e-02, -3.9108e-01,\n",
       "                       -1.5261e+00, -2.0880e-01, -2.1027e-01, -4.8048e-01,  2.2296e-03,\n",
       "                       -6.9192e-01, -1.0822e-01, -6.8506e-01],\n",
       "                      [-9.3650e-02, -7.8182e-02,  1.6746e-01, -5.1328e-01, -2.5913e-01,\n",
       "                       -1.2394e-01,  3.3046e-03,  1.5678e-02, -3.1828e-01, -3.8878e-02,\n",
       "                       -1.2032e-01, -1.0228e+00, -2.2920e-01, -2.4052e-01, -3.7711e-01,\n",
       "                       -2.2572e-01, -5.8813e-01,  4.4079e-02,  8.8769e-03, -2.2573e-01,\n",
       "                       -8.8563e-02, -9.5968e-01, -1.6330e-01, -8.5558e-02,  1.2996e-01,\n",
       "                       -1.2464e-01, -9.0285e-02, -8.7719e-01, -8.6169e-02, -2.4423e-01,\n",
       "                       -3.7963e-02, -9.7258e-01, -1.0802e-01, -7.6795e-01,  5.0003e-02,\n",
       "                       -8.0017e-02, -1.4118e+00, -8.6950e-02, -1.9223e-01, -4.0354e-02,\n",
       "                        1.0487e-01, -9.5239e-01, -6.1048e-02, -2.2344e-01, -3.6116e-01,\n",
       "                        8.9745e-02, -2.2477e-01, -1.2811e-01, -1.2403e-01, -6.3335e-01,\n",
       "                        1.7138e-01, -2.0539e-01, -4.1232e-01, -1.1157e-01, -7.1585e-01,\n",
       "                       -1.2909e-01, -2.6255e-01, -1.9110e-01,  6.1177e-02,  1.2728e-01,\n",
       "                       -2.4279e-02, -9.8455e-02, -3.6677e-01,  8.4629e-02,  1.8920e-02,\n",
       "                       -5.3896e-01, -3.3532e-01, -7.4264e-01, -2.1083e-01, -6.8004e-01,\n",
       "                       -4.1480e-01, -4.3680e-01, -2.4679e-01, -3.1155e-02, -7.0648e-01,\n",
       "                        1.0012e-01, -1.2141e-01, -3.3740e-01, -2.9262e-01, -1.3173e-01,\n",
       "                       -9.0065e-02,  9.7317e-02, -9.1439e-02, -1.2286e+00, -3.1423e-02,\n",
       "                       -9.5043e-02, -3.8439e-01, -7.6019e-01, -2.7176e-01, -1.5717e-01,\n",
       "                       -3.0722e-01, -6.0240e-04, -1.0484e-01, -1.8567e-01, -1.6258e-01,\n",
       "                       -8.8318e-02, -2.7314e-01, -6.0577e-01],\n",
       "                      [-1.6300e-01, -3.6294e-01,  8.9986e-02, -3.6023e-01, -3.6626e-01,\n",
       "                       -5.4134e-01,  1.0226e-01,  9.0529e-02,  1.4383e-01,  5.8084e-02,\n",
       "                        8.9960e-02, -6.4538e-01, -7.6297e-01,  1.7699e-01, -4.1569e-01,\n",
       "                       -4.8092e-01, -2.3735e-01, -2.9837e-01, -1.4851e-01, -4.7514e-01,\n",
       "                       -1.0308e+00, -6.9044e-01, -8.0262e-03, -3.4620e-01,  4.7659e-02,\n",
       "                       -6.7840e-01,  6.4724e-02, -4.2748e-01,  7.8682e-02, -4.1671e-01,\n",
       "                        4.3712e-02, -2.6536e-01, -1.7028e-01, -1.2953e-01,  1.9866e-01,\n",
       "                       -1.6395e-02, -1.0954e+00,  5.7162e-02, -1.5948e-01, -1.1624e-01,\n",
       "                       -4.8549e-01, -6.2810e-01, -7.9036e-02, -1.5076e-01, -2.0655e-01,\n",
       "                       -4.5365e-02, -5.4219e-01, -7.4769e-02, -4.6832e-01, -8.9183e-01,\n",
       "                       -1.4947e-02, -2.0346e-01, -5.9781e-01,  2.5661e-02, -1.2298e+00,\n",
       "                       -2.3181e-01, -1.7309e-01,  1.6108e-01, -1.0631e-01, -9.5859e-02,\n",
       "                       -4.3472e-02, -3.5835e-01,  1.9752e-01, -3.0542e-01,  2.3873e-02,\n",
       "                       -2.5896e-01, -3.7343e-01,  8.8047e-02, -2.4699e-01, -4.5823e-01,\n",
       "                       -8.6308e-01, -7.3993e-02, -6.1307e-01, -8.1291e-01, -5.7573e-01,\n",
       "                        9.6744e-03, -4.5415e-01, -4.7822e-01, -7.0348e-02, -1.9904e-01,\n",
       "                        3.9884e-02, -5.2157e-01, -2.5398e-01, -4.4461e-01, -3.1985e-01,\n",
       "                        7.4058e-02, -3.0087e-01, -1.7025e+00, -3.5473e-01, -2.6142e-01,\n",
       "                       -1.2369e+00, -1.1328e-01, -2.4021e-01, -6.4845e-01, -2.3720e-01,\n",
       "                       -2.1335e-01, -1.8430e-01, -1.0569e+00]])),\n",
       "             ('o.bias',\n",
       "              tensor([-0.1808, -2.1373,  0.3520,  0.2080,  0.2256, -0.5331,  0.3357,  0.1825,\n",
       "                       0.6976, -0.7297]))])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['h1.weight', 'h1.bias', 'h2.weight', 'h2.bias', 'h3.weight', 'h3.bias', 'o.weight', 'o.bias'])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model's state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Network(\n",
       "  (h1): Linear(in_features=784, out_features=392, bias=True)\n",
       "  (h2): Linear(in_features=392, out_features=196, bias=True)\n",
       "  (h3): Linear(in_features=196, out_features=98, bias=True)\n",
       "  (o): Linear(in_features=98, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.2)\n",
       ")>"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = {'input': 784,\n",
    "                   'output': 10,\n",
    "                   'hidden': [392,196,98],\n",
    "                   'state_dict':model.state_dict()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_checkpoint,'save_model_exercise.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('save_model_exercise.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 784,\n",
       " 'output': 10,\n",
       " 'hidden': [392, 196, 98],\n",
       " 'state_dict': OrderedDict([('h1.weight',\n",
       "               tensor([[ 0.6635,  0.6317,  0.6783,  ...,  0.0347,  0.4029,  0.6043],\n",
       "                       [ 0.6620,  0.6720,  0.7170,  ...,  0.7780,  0.6932,  0.6519],\n",
       "                       [ 0.2829,  0.2228,  0.2746,  ...,  0.2472,  0.3564,  0.2465],\n",
       "                       ...,\n",
       "                       [ 0.1973,  0.2188,  0.2075,  ..., -0.2337, -0.0223,  0.1776],\n",
       "                       [ 0.1578,  0.1640,  0.1244,  ...,  0.1436,  0.1222,  0.1421],\n",
       "                       [ 0.0838,  0.0753,  0.0645,  ...,  0.0424,  0.0509,  0.0721]])),\n",
       "              ('h1.bias',\n",
       "               tensor([-0.6652, -0.6343, -0.2729, -0.0999, -0.3086, -0.2899, -0.1437, -0.0717,\n",
       "                       -0.7657, -0.3900, -0.0302, -0.4355, -0.2352, -0.4811, -0.1490, -0.0904,\n",
       "                       -0.2654, -0.0993, -0.5458, -0.0679, -0.2055, -0.1234, -0.0891, -0.1885,\n",
       "                       -0.1709,  0.3557, -0.1333, -0.2390, -0.3208, -0.2140, -0.3272, -0.6511,\n",
       "                       -0.6942, -0.0365, -0.5369, -0.4165, -0.1422, -0.0607, -0.2982, -0.3450,\n",
       "                       -0.1518, -0.0486, -0.1016, -0.2103, -0.1785, -0.0715, -0.0710, -0.3402,\n",
       "                       -0.0289, -0.2843, -0.0142, -0.7158, -0.6437, -0.2483, -0.2013, -0.2433,\n",
       "                       -0.0262, -0.4421, -0.2980, -0.4082, -0.0726, -0.1578, -0.3099, -0.0649,\n",
       "                       -0.1839,  0.0849, -0.1520, -0.3759, -0.2398, -0.7390, -0.4593, -0.2062,\n",
       "                       -0.0609, -0.3272, -0.0539, -0.4661, -0.2356, -0.5939, -0.3882, -0.0620,\n",
       "                       -0.1108, -0.4723, -0.2968, -0.2679, -0.1024,  0.0081, -0.4644, -0.2239,\n",
       "                       -0.1163, -0.3931, -0.2697, -0.2283, -0.0491, -0.3264, -0.2977, -0.0609,\n",
       "                       -0.4770, -0.2940, -0.1259, -0.5155, -0.4434, -0.4120, -0.3915, -0.5054,\n",
       "                       -0.0353, -0.3582, -0.0368, -0.1525, -0.4445, -0.0615, -0.0203, -0.1636,\n",
       "                       -0.3175, -0.0716, -0.1618, -0.1284, -0.3572, -0.2520, -0.3190, -0.2942,\n",
       "                       -0.2514, -0.2606, -0.1032, -0.4153, -0.0398, -0.1287, -0.7525, -0.0329,\n",
       "                       -0.4573, -0.4696, -0.2153, -0.5301, -0.2339, -0.3366, -0.3475, -0.2809,\n",
       "                       -0.3083, -0.2132, -0.2176, -0.0314, -0.2534, -0.1936, -0.2967, -0.1427,\n",
       "                       -0.1879, -0.2152, -0.5503, -0.4905, -0.0588, -0.9600, -0.1334, -0.4662,\n",
       "                       -0.0784, -0.2347, -0.0624, -0.3125, -0.4618, -0.2208, -0.2367, -0.1110,\n",
       "                       -0.4293, -0.3638, -0.4762, -0.0390, -0.5621, -0.1030, -0.1350, -0.0641,\n",
       "                       -0.0497, -0.3087, -0.0543, -0.2590, -0.0433, -0.4001, -0.5347, -0.0935,\n",
       "                       -0.3482, -0.1368, -0.1378, -0.2452, -0.0220, -0.3026, -0.0198, -0.0809,\n",
       "                       -0.3547, -0.0747, -0.0440, -0.3120, -0.5478, -0.5646, -0.0545, -0.3461,\n",
       "                       -0.2895, -0.0457, -0.0560, -0.4933, -0.3211, -0.5336, -0.5070, -0.3196,\n",
       "                       -0.1228, -0.5380, -0.0823, -0.1181, -0.1611, -0.2395, -0.2008, -0.3780,\n",
       "                       -0.1372, -0.1253, -0.2524, -0.2358, -0.1016, -0.1355, -0.0953, -0.0606,\n",
       "                       -0.0657, -0.2033, -0.0691, -0.2129, -0.3467, -0.4505, -0.3327, -0.1521,\n",
       "                       -0.3058, -0.0696, -0.1257, -0.0596, -0.0504, -0.6527, -0.0560, -0.0825,\n",
       "                       -0.4122, -0.0932, -0.1762, -0.0349, -0.3610, -0.5102, -0.1689, -0.1601,\n",
       "                       -0.6920, -0.0303, -0.2095, -0.8167, -0.0353, -0.1433, -0.0739, -0.2536,\n",
       "                       -0.5781, -0.4523, -0.0737, -0.0659, -0.5063, -0.3581, -0.0707, -0.0379,\n",
       "                       -0.3493, -0.1870, -0.2328, -0.0393, -0.1546, -0.4214, -0.1542, -0.1678,\n",
       "                       -0.6983, -0.5797,  0.2270, -0.3274, -0.3906, -0.2832, -0.0707, -0.3137,\n",
       "                       -0.4720, -0.1595, -0.4646, -0.1647, -0.6295, -0.2926, -0.4405, -0.4155,\n",
       "                       -0.1050, -0.0125, -0.3055, -0.0325, -0.2390, -0.2489, -0.3486, -0.0882,\n",
       "                       -0.2954, -0.0709, -0.4963, -0.1234, -0.4749, -0.2405, -0.0898, -0.2974,\n",
       "                       -0.0995, -0.1967, -0.0083, -0.6662, -0.1281, -0.1596, -0.1818, -0.4328,\n",
       "                       -0.3845, -0.1117, -0.0214, -0.4282, -0.7230, -0.4727, -0.1357, -0.3084,\n",
       "                       -0.1518, -0.1055, -0.5030, -0.0963, -0.5729, -0.1482, -0.5832,  0.7324,\n",
       "                       -0.1581, -0.0736, -0.2557, -0.2326, -0.1663, -0.1622, -0.1095, -0.3878,\n",
       "                       -0.2923, -0.6673, -0.6394, -0.3599, -0.1837, -0.2737, -0.6059, -0.1647,\n",
       "                       -0.2773, -0.0668, -0.0739, -0.4045, -0.3202, -0.2920, -0.2786, -0.2590,\n",
       "                       -0.3034, -0.4192, -0.1504, -0.6600, -0.0866, -0.2178, -0.0438, -0.3919,\n",
       "                       -0.1052, -0.1762, -0.0518, -0.3757, -0.1423, -0.0486, -0.7486, -0.0547,\n",
       "                       -0.0796, -0.1979, -0.3888, -0.3032, -0.1514, -0.1781, -0.0064, -0.7095,\n",
       "                       -0.3626, -0.0733, -0.2351, -0.4623, -0.2885, -0.4197, -0.4444, -0.0898,\n",
       "                       -0.3687, -0.2194, -0.0373, -0.0729, -0.3579, -0.3036, -0.1971, -0.0755,\n",
       "                       -0.2651, -0.0710, -0.0733, -0.2321, -0.0422, -0.2395, -0.1271, -0.0765])),\n",
       "              ('h2.weight',\n",
       "               tensor([[ 0.5665, -0.6494,  0.1304,  ...,  0.1758,  0.0131, -0.0154],\n",
       "                       [-0.5531, -0.7338, -0.2200,  ..., -0.0295,  0.0367, -0.0637],\n",
       "                       [ 0.0379, -0.1093, -0.9919,  ..., -0.4423,  0.2343, -0.0947],\n",
       "                       ...,\n",
       "                       [ 0.2178, -0.3875, -0.1372,  ..., -1.0426, -0.2529,  0.0154],\n",
       "                       [ 0.6982, -0.7661, -0.4249,  ..., -0.4155, -0.0473,  0.0195],\n",
       "                       [-0.3789, -0.3805, -0.3798,  ..., -0.6692,  0.1531,  0.0258]])),\n",
       "              ('h2.bias',\n",
       "               tensor([-1.5435e+00, -1.3552e+00,  1.0399e-01, -2.3664e+00, -1.7831e+00,\n",
       "                       -3.1740e+00, -1.1657e+00, -6.3832e-02, -6.4369e-01,  2.3912e-02,\n",
       "                       -4.3545e-01, -2.7548e+00, -2.4808e+00,  1.5373e-01, -1.5721e+00,\n",
       "                        1.5400e-01, -1.3202e-01,  8.0410e-01, -3.3362e-02, -2.7887e+00,\n",
       "                       -5.6820e-01,  4.6902e-01, -3.2990e+00, -2.1378e+00, -1.6258e+00,\n",
       "                       -2.5671e+00, -1.8737e+00, -3.2015e+00, -1.2941e+00, -2.1534e+00,\n",
       "                       -2.3061e+00, -2.6533e+00, -1.8039e-01,  2.2461e-01,  3.8681e-01,\n",
       "                        2.1081e-01, -3.3864e+00, -3.5205e+00,  9.4573e-01,  8.2843e-01,\n",
       "                       -2.7083e+00, -1.9377e+00,  4.3036e-01, -2.0884e+00, -8.3240e-01,\n",
       "                       -2.1255e+00, -1.6126e+00, -3.9483e+00, -3.6561e+00, -4.2140e-01,\n",
       "                       -1.7253e+00, -2.6481e+00, -1.6821e+00, -3.2526e+00, -3.1900e+00,\n",
       "                       -2.6005e-02, -2.4487e+00, -2.9820e+00, -1.4561e+00, -1.4447e-01,\n",
       "                       -2.0812e+00, -6.8057e-01, -2.1239e-01, -2.6893e+00, -1.0484e+00,\n",
       "                       -1.9314e+00, -2.4547e+00, -2.3055e+00, -1.8538e+00, -2.8168e+00,\n",
       "                       -7.6832e-01, -3.5595e+00, -2.0689e+00, -1.6527e-01, -2.2698e+00,\n",
       "                        1.2675e-01, -3.3544e+00, -1.5878e+00, -8.9925e-01, -5.6601e-01,\n",
       "                       -3.6768e+00, -1.8327e+00, -2.3505e+00, -1.6881e-01, -3.2961e+00,\n",
       "                       -2.0170e+00, -2.5633e+00, -1.9024e+00, -2.9769e+00, -1.2003e-03,\n",
       "                       -1.1279e+00, -3.5646e+00, -3.8145e+00, -2.0198e+00, -1.3489e+00,\n",
       "                       -2.9413e+00, -1.1764e+00, -3.3401e+00, -1.6785e+00, -1.4654e+00,\n",
       "                       -1.7755e-01, -1.2151e+00, -3.7652e+00, -2.5707e+00, -5.5560e-02,\n",
       "                       -2.3769e+00,  1.1667e-01,  5.5293e-01, -2.2245e+00, -1.1566e+00,\n",
       "                        1.6741e-01,  6.7772e-01, -2.5196e+00, -7.5379e-01, -3.0700e+00,\n",
       "                       -2.5824e+00, -1.9775e+00, -1.4699e+00, -1.4700e-01, -2.1954e+00,\n",
       "                       -3.2470e+00, -5.4336e-02, -8.1316e-01,  2.9839e-01, -2.1294e+00,\n",
       "                       -2.8617e+00, -1.2877e+00, -1.5665e+00, -2.7253e+00, -1.9623e+00,\n",
       "                       -2.8144e+00, -1.8026e+00, -2.9227e+00,  3.9764e-01,  1.8676e-01,\n",
       "                        5.1454e-01, -3.5664e+00, -3.4575e+00, -1.6658e+00,  7.3666e-01,\n",
       "                       -2.0308e+00, -5.4133e-02, -1.6065e+00,  8.6995e-01, -2.1704e+00,\n",
       "                       -3.4339e+00, -3.0888e+00, -3.4942e-03, -3.3668e+00, -2.2128e+00,\n",
       "                       -6.7904e-01, -3.2639e+00, -5.3756e-01, -3.6950e+00, -9.5049e-01,\n",
       "                        6.7204e-01, -1.1001e+00, -2.3119e+00, -2.0995e+00, -2.1756e+00,\n",
       "                       -3.3381e+00, -1.6310e+00, -4.0523e+00, -1.4200e+00, -1.0008e+00,\n",
       "                       -2.1644e-02, -4.6473e-01, -1.8164e-01, -1.8773e+00,  5.2176e-01,\n",
       "                       -1.1146e+00, -2.1544e+00, -1.6444e+00, -9.5193e-02, -2.3508e+00,\n",
       "                       -3.3461e-01, -6.1814e-01, -3.0467e+00, -1.9458e+00,  2.2709e-01,\n",
       "                        4.1700e-01, -3.5464e+00, -8.4255e-01, -7.9023e-02, -2.7858e+00,\n",
       "                        1.9467e-01,  3.8069e-01, -2.4473e-02,  8.0008e-01, -2.3495e+00,\n",
       "                       -1.4914e+00, -3.0675e+00, -3.0546e+00, -2.1270e+00, -2.8242e+00,\n",
       "                       -9.3915e-01])),\n",
       "              ('h3.weight',\n",
       "               tensor([[-0.4110, -0.2564, -0.5559,  ..., -0.1928,  0.0416, -0.5057],\n",
       "                       [-0.2140, -0.0286, -0.7471,  ..., -0.2477, -0.1089,  0.1631],\n",
       "                       [-0.1266, -0.2885, -0.1877,  ...,  0.3731,  0.1129, -0.5543],\n",
       "                       ...,\n",
       "                       [-0.4008, -0.1925, -1.0479,  ..., -0.3716, -0.6747,  0.0055],\n",
       "                       [-0.1909, -0.2211, -1.0243,  ..., -0.3000, -0.1023, -0.2158],\n",
       "                       [-0.7579, -0.4841, -0.0871,  ..., -0.1872, -0.3491,  0.4415]])),\n",
       "              ('h3.bias',\n",
       "               tensor([-1.9270,  1.3259, -0.6944, -3.6528, -3.5744,  1.6230, -1.8968,  0.3914,\n",
       "                        1.0385,  2.4112,  0.9177, -3.3913,  1.3021,  0.9741,  0.8355,  1.2464,\n",
       "                       -2.6403, -2.7420, -0.2084, -3.2952, -0.8027, -2.9395, -2.0222,  0.6824,\n",
       "                        0.5230,  1.1106, -1.6171, -3.4816,  2.4350,  1.7452,  0.9124, -1.9960,\n",
       "                        0.4311, -2.3804, -0.1778, -0.8650, -2.2876,  0.0601,  1.0442, -1.3451,\n",
       "                       -2.4511, -2.4920, -3.3654, -2.8596, -2.9755,  0.1576,  0.8388, -2.5850,\n",
       "                       -1.7503, -3.2069, -2.2482,  1.6152, -2.0272,  1.0155, -1.8515,  1.6564,\n",
       "                        1.8630,  1.0332, -3.5183,  1.5280,  0.9362,  1.0355,  0.7870, -2.9110,\n",
       "                        1.1668, -3.0438, -2.4818, -3.2503, -2.2327, -2.6998, -2.4905,  0.3789,\n",
       "                        0.9276,  0.6142, -1.7600, -1.0963, -3.0667,  1.2790,  0.3929,  1.0461,\n",
       "                        0.7603, -1.5717, -2.0507, -3.1856, -3.3989,  1.2182, -2.1285, -3.2191,\n",
       "                       -2.9655,  1.9186, -3.2449, -2.5403,  1.2877,  0.8484, -2.8324,  0.9992,\n",
       "                       -1.7133, -3.2218])),\n",
       "              ('o.weight',\n",
       "               tensor([[-1.9234e-01, -2.0392e-02,  1.0481e-01, -8.4606e-01, -2.4985e-01,\n",
       "                        -3.0678e-04, -3.8632e-02,  5.8521e-02, -3.3488e-01, -1.2923e-01,\n",
       "                        -4.1323e-01, -5.3015e-01, -2.5373e-01, -3.8867e-01, -1.8995e-01,\n",
       "                        -6.4196e-02, -8.3333e-01, -4.0994e-01,  6.8739e-02, -1.7357e-01,\n",
       "                         5.8822e-02, -1.4631e+00, -2.7855e-01,  1.1102e-01,  1.0113e-01,\n",
       "                        -1.8852e-02, -3.9692e-01, -3.0763e-01, -2.0068e-01,  1.0765e-01,\n",
       "                        -5.5327e-02,  1.4322e-01, -5.9226e-02, -4.5592e-01,  1.0582e-02,\n",
       "                        -1.5212e-01, -7.6006e-01, -1.1964e-02, -3.9285e-02, -1.6659e-01,\n",
       "                        -9.0559e-02, -3.2806e-01, -4.1844e-01, -3.2178e-01, -5.2669e-01,\n",
       "                         4.1047e-02, -3.5199e-01, -7.1059e-01, -3.4225e-01, -1.1065e+00,\n",
       "                        -4.3778e-01,  1.0340e-01, -4.6064e-01,  1.6318e-04, -4.1228e-01,\n",
       "                         1.1867e-01, -6.0195e-02, -6.6756e-01, -1.0632e-01,  1.1791e-01,\n",
       "                         3.4648e-02, -3.2833e-01, -4.1097e-01, -4.4120e-01, -2.1580e-01,\n",
       "                        -8.0816e-01, -3.9806e-01, -3.6816e-01,  2.1238e-01, -3.9080e-01,\n",
       "                        -1.8546e-02, -2.1229e-01,  8.3097e-02,  6.0822e-02, -1.8401e-02,\n",
       "                        -1.2010e-01, -3.9016e-01,  1.6120e-01, -1.2258e-01, -1.3141e-01,\n",
       "                        -2.9459e-01, -9.4209e-02, -4.5429e-01, -1.9602e-01, -1.3236e-01,\n",
       "                        -3.6590e-01, -1.5397e-02, -1.8814e-01,  5.1120e-02, -1.7046e-01,\n",
       "                        -1.6578e-01, -4.2406e-01,  6.3101e-03, -2.2364e-01, -1.5229e-01,\n",
       "                        -5.0592e-02, -2.6943e-01, -1.7907e-01],\n",
       "                       [-2.0223e-02,  9.3189e-02, -1.2809e-02, -5.6003e-01, -2.7675e-01,\n",
       "                        -1.3911e-01, -4.6306e-01,  1.1617e-01, -3.3544e-01, -1.5588e-01,\n",
       "                        -3.7080e-01, -6.1663e-01, -1.3931e-01, -5.0355e-01, -2.0685e-01,\n",
       "                         7.5613e-02, -7.1284e-01, -6.1171e-01,  2.0607e-02, -7.3848e-01,\n",
       "                         3.6322e-01, -9.3607e-01, -4.7486e-01,  1.0096e-01,  4.0209e-02,\n",
       "                        -2.9974e-02, -3.3240e-01, -7.4555e-01, -1.8913e-02, -4.4221e-02,\n",
       "                         9.9278e-02, -4.2486e-01, -3.3746e-01,  6.1161e-02, -1.1168e-02,\n",
       "                        -2.0361e-02, -6.2058e-01, -6.7045e-02, -3.0847e-02, -4.7677e-01,\n",
       "                        -6.7087e-02, -1.1269e-01, -2.2388e-01, -2.9809e-01, -4.3837e-01,\n",
       "                        -1.1624e-01, -1.3669e-01, -1.8059e-01, -3.6600e-01, -6.0429e-01,\n",
       "                        -7.1404e-01,  8.7330e-02, -2.2463e-01,  2.2545e-01, -1.7390e-01,\n",
       "                         7.5932e-02,  8.6393e-02, -3.2723e-01, -5.1989e-02, -8.0292e-02,\n",
       "                        -1.7509e-01, -2.0744e-01, -8.7897e-02, -1.5312e-01, -2.5382e-01,\n",
       "                        -2.0326e-01, -1.0581e+00, -2.9085e-01, -2.8910e-01, -4.9349e-01,\n",
       "                        -1.7578e-01,  2.7743e-02,  4.7116e-02, -1.0047e-01, -3.1654e-01,\n",
       "                         1.2703e-01, -2.6230e-02, -4.3751e-02, -2.3436e-01, -1.1837e-01,\n",
       "                        -2.9399e-01,  5.8717e-02, -4.0129e-01, -2.8931e-01, -5.2082e-02,\n",
       "                        -3.8528e-01,  1.3005e-01, -1.8831e-01, -2.7081e-01,  3.4468e-02,\n",
       "                        -2.7875e-01, -3.6907e-01,  1.4438e-01, -9.6696e-02, -1.0317e-01,\n",
       "                         7.4305e-02, -1.0316e+00, -1.6450e-01],\n",
       "                       [-2.6956e-01, -1.5862e-02,  8.5707e-02, -5.7867e-01, -2.0022e-01,\n",
       "                         1.0339e-01,  5.3706e-02, -2.0057e-02, -4.0386e-01,  6.7846e-02,\n",
       "                        -6.3963e-01, -5.1776e-01,  6.9138e-02, -4.8486e-01,  1.7856e-03,\n",
       "                        -1.9906e-02, -5.1098e-01, -4.8878e-01,  9.7062e-03, -3.3626e-01,\n",
       "                        -2.4908e-02, -1.1058e+00, -2.3607e-01,  1.2351e-01,  8.6847e-02,\n",
       "                         1.6635e-03, -2.9353e-01, -3.4212e-01, -3.3395e-01,  3.7120e-02,\n",
       "                        -3.2757e-01, -2.5764e-01, -8.4581e-03, -5.9763e-01,  1.4922e-02,\n",
       "                        -4.4555e-01, -1.0269e+00, -2.0787e-01,  4.9417e-02, -1.6869e-01,\n",
       "                         7.7742e-02, -2.7115e-01, -2.2909e-01, -1.7801e-01, -3.5184e-02,\n",
       "                        -6.8503e-02, -6.5730e-02, -2.3191e-01, -2.5156e-01, -6.9111e-01,\n",
       "                        -5.1933e-01, -9.7853e-02, -4.1939e-01,  1.6304e-01, -2.8365e-01,\n",
       "                        -2.2582e-01,  7.0591e-02, -5.4872e-01, -6.2038e-01, -4.1959e-02,\n",
       "                        -7.4756e-02,  1.3981e-02, -4.5751e-01, -5.0494e-01, -2.3401e-01,\n",
       "                        -3.9140e-01, -5.1935e-01, -4.0881e-01, -1.4238e-01, -1.7415e-01,\n",
       "                        -1.0550e-01, -2.6113e-01, -1.5253e-01, -4.5861e-02, -1.5113e-01,\n",
       "                        -4.7604e-02, -2.0269e-01, -1.0011e-01,  1.0945e-03,  5.2888e-02,\n",
       "                        -5.3868e-01, -1.2321e-01, -1.2001e-01, -3.0595e-01, -2.1236e-01,\n",
       "                        -5.9749e-01, -8.2944e-02, -1.3409e-01, -3.1764e-01,  5.8371e-02,\n",
       "                        -4.8557e-01, -4.9142e-01, -8.2778e-02,  7.1244e-02,  9.5528e-03,\n",
       "                        -1.1882e-01, -2.2129e-01, -5.1356e-01],\n",
       "                       [ 2.7610e-01,  8.7969e-02,  2.3888e-02, -8.0481e-01, -1.7688e-01,\n",
       "                        -1.0676e-01,  1.6712e-01,  3.9101e-02, -2.2290e-01, -5.8436e-02,\n",
       "                        -2.5520e-01, -4.9240e-01, -1.8983e-01, -4.7088e-01, -4.0033e-01,\n",
       "                         5.2619e-03, -3.4719e-01, -3.0246e-01, -3.7529e-02, -9.3895e-01,\n",
       "                         2.1785e-01, -8.4805e-01,  2.1099e-01, -9.7565e-02, -3.7044e-02,\n",
       "                         1.1098e-01, -2.7207e-01, -3.6735e-01, -2.9909e-01, -7.0785e-02,\n",
       "                        -7.9227e-02, -1.3951e-01, -3.8361e-01, -2.0609e-01, -3.8996e-01,\n",
       "                        -1.1692e-01, -2.5025e-01, -2.2281e-01, -6.2944e-02, -8.0971e-01,\n",
       "                        -1.0824e-01, -5.3167e-01, -4.5228e-01, -1.9230e-01, -2.9704e-01,\n",
       "                        -7.4755e-02,  1.9265e-02, -2.0599e-01, -5.1775e-01, -6.4292e-01,\n",
       "                        -2.9001e-01,  1.5398e-01, -3.5206e-01,  4.4484e-02, -2.4055e-01,\n",
       "                        -3.7969e-03, -1.0280e-01, -5.1669e-01, -1.4132e-01, -9.9336e-02,\n",
       "                        -1.2007e-01, -2.1444e-01, -2.8022e-01, -8.6720e-02, -3.6152e-01,\n",
       "                        -7.7595e-01, -6.3854e-01, -4.3540e-01, -3.7993e-01, -4.6383e-01,\n",
       "                        -2.8100e-02, -4.5796e-02,  1.0039e-01, -9.1461e-02, -4.4954e-02,\n",
       "                        -4.4241e-02,  7.3695e-02,  6.3592e-03, -1.3515e-01, -3.3294e-01,\n",
       "                        -3.0844e-01, -2.1980e-02, -1.6039e-01, -3.3565e-01, -2.1751e-01,\n",
       "                        -3.8988e-01,  1.2752e-01, -2.1912e-01, -5.2645e-01, -3.6088e-02,\n",
       "                        -1.6002e-01, -4.8878e-01,  1.4171e-01,  9.4838e-02, -3.9430e-01,\n",
       "                         1.0585e-01, -1.2203e+00, -3.6481e-01],\n",
       "                       [ 2.5529e-02,  8.1359e-03,  4.7088e-02, -6.6894e-01, -2.0471e-01,\n",
       "                        -1.7118e-02,  9.4947e-02, -1.0728e-02, -3.5267e-01, -4.6667e-02,\n",
       "                        -4.8586e-01, -5.5204e-01,  1.2777e-01, -4.6390e-01,  2.5457e-02,\n",
       "                         4.9312e-02, -3.6495e-01, -2.9378e-01, -1.2901e-01, -3.4605e-01,\n",
       "                        -9.6697e-02, -6.6070e-01, -6.9578e-02,  1.5088e-02,  8.0132e-02,\n",
       "                         1.0356e-01, -2.3029e-01, -4.2995e-01, -2.8614e-01, -1.6389e-01,\n",
       "                        -1.1691e-01, -1.9463e-01,  1.0515e-01, -1.8765e-01,  1.0264e-01,\n",
       "                        -3.6632e-01, -5.2143e-01, -1.7667e-01,  9.3417e-02, -1.7389e-01,\n",
       "                        -1.3172e-01, -7.4512e-01, -7.0437e-01, -1.9446e-01, -4.8076e-01,\n",
       "                        -3.4790e-02,  8.4649e-02, -9.7472e-02, -3.4022e-01, -1.1572e+00,\n",
       "                        -5.6891e-01, -1.3316e-01, -2.7414e-01, -3.3570e-02, -5.1385e-01,\n",
       "                        -2.2037e-01,  3.9476e-02, -4.1176e-01,  2.4985e-02, -4.9499e-02,\n",
       "                        -2.7956e-01,  6.4092e-02, -3.4626e-01, -9.6991e-01, -2.4206e-01,\n",
       "                        -4.8220e-01, -9.1850e-01, -4.5574e-01, -2.5293e-01, -2.9793e-01,\n",
       "                         2.0000e-02,  5.5788e-02, -1.8126e-01,  3.1386e-02, -3.0474e-01,\n",
       "                         2.9063e-02, -1.1559e-02, -7.7304e-02,  6.8082e-02,  3.8522e-02,\n",
       "                        -4.8098e-01, -1.3494e-01, -1.9621e-01, -2.4482e-01, -1.2186e-01,\n",
       "                        -3.2855e-01, -7.5455e-01, -2.6855e-01, -3.0145e-01,  4.3826e-02,\n",
       "                        -1.7124e-01, -4.0642e-01,  5.6571e-02,  1.3135e-01, -7.6231e-02,\n",
       "                        -1.0629e-01, -3.1276e-01, -4.4139e-01],\n",
       "                       [-9.1310e-02, -1.9628e-01, -1.3181e-02, -2.0604e-01, -7.3838e-01,\n",
       "                        -1.6279e-01, -2.0942e-01,  1.5013e-01,  1.2487e-01,  1.3629e-01,\n",
       "                        -4.0493e-03, -6.4569e-01, -1.1655e-01,  7.4777e-02, -6.0700e-01,\n",
       "                        -3.6555e-01, -2.5439e-01, -4.5057e-02, -2.9257e-03, -7.7618e-02,\n",
       "                        -2.1926e-01, -7.6314e-01, -4.6268e-01, -1.7963e-01, -3.6202e-04,\n",
       "                        -3.0043e-01, -1.6356e-01, -3.1252e-01,  1.6627e-01, -2.1943e-01,\n",
       "                         1.0981e-01, -2.1019e-01, -1.9202e-01,  2.9213e-02,  1.3316e-02,\n",
       "                         5.0107e-02, -6.8600e-01,  2.0551e-01, -2.5352e-01,  6.2030e-02,\n",
       "                        -5.6650e-01, -6.4165e-01, -6.8429e-02, -6.8076e-02,  1.3316e-02,\n",
       "                         7.3908e-02, -5.4190e-01, -1.7307e-01, -6.0448e-01, -5.8795e-01,\n",
       "                        -1.5463e-01, -1.6271e-01, -4.8793e-01, -6.8105e-02, -1.2771e-01,\n",
       "                        -3.4476e-01, -2.9775e-01,  7.2811e-02, -2.2257e-01,  8.2680e-02,\n",
       "                         1.6000e-01, -3.7352e-01,  9.1123e-02, -3.2200e-01,  1.2165e-01,\n",
       "                        -6.7461e-02, -5.5125e-01, -3.2387e-01, -1.1077e+00,  7.8804e-02,\n",
       "                        -4.5879e-01,  1.0958e-02, -1.8659e-01, -3.8021e-01, -4.5693e-01,\n",
       "                        -1.4894e-01, -5.9550e-01, -1.3716e-01, -2.3569e-01, -4.8260e-01,\n",
       "                         4.4806e-02, -1.5770e-01, -2.2595e-01, -3.1467e-01, -1.8914e-01,\n",
       "                         5.1954e-02, -1.6116e-01, -9.8763e-01, -8.7075e-02, -2.1828e-01,\n",
       "                        -9.6606e-01, -3.3996e-01, -2.4010e-01, -4.1518e-01, -1.2740e-01,\n",
       "                        -7.0925e-02, -1.1647e-01, -7.6939e-01],\n",
       "                       [-1.2326e-01, -9.1631e-02,  1.6377e-01, -3.9861e-01, -3.3557e-01,\n",
       "                         1.0156e-01,  5.5104e-02,  4.5794e-02, -3.4659e-01,  3.4519e-02,\n",
       "                        -3.9397e-01, -4.2861e-01,  1.0816e-01, -4.5315e-01, -7.9866e-02,\n",
       "                        -5.1686e-02, -5.0692e-01, -4.1810e-01, -5.1410e-02, -8.9267e-02,\n",
       "                         3.2152e-02, -9.6017e-01, -3.7393e-01,  9.7063e-02,  9.7520e-02,\n",
       "                         3.0622e-02, -3.6101e-01, -4.8962e-01, -2.6188e-01,  1.0960e-01,\n",
       "                        -1.3460e-01, -1.8141e-01,  3.4836e-02, -2.1468e-01,  2.8970e-03,\n",
       "                        -1.4874e-01, -7.3836e-01, -5.9450e-02,  1.8391e-01, -2.1586e-01,\n",
       "                        -1.4733e-01, -2.5778e-01, -3.0486e-01, -1.8602e-01, -7.8020e-01,\n",
       "                         2.0473e-02,  4.3595e-02, -2.0660e-01, -3.6191e-01, -7.7239e-01,\n",
       "                        -4.4940e-01,  9.2762e-02, -1.7107e-01, -3.6253e-02, -4.3444e-01,\n",
       "                         4.1781e-02,  3.7009e-02, -7.6080e-01, -1.0528e-01,  1.0524e-01,\n",
       "                        -2.6246e-01, -9.4286e-02, -2.5098e-01, -4.4294e-01, -1.9654e-01,\n",
       "                        -2.8053e-01, -5.7387e-01, -2.7590e-01, -2.3916e-01, -4.0527e-01,\n",
       "                        -5.3715e-02, -1.3205e-01,  5.4399e-02,  1.3792e-01, -3.3712e-02,\n",
       "                         1.0187e-01, -1.5057e-01,  1.1309e-01,  1.4072e-01, -2.1243e-02,\n",
       "                        -3.3966e-01, -3.1909e-02, -9.9037e-02, -1.4681e-01, -1.3715e-01,\n",
       "                        -5.7588e-01,  1.3991e-01, -1.4633e-01, -4.5083e-01, -1.9651e-02,\n",
       "                        -2.4675e-01, -3.4044e-01, -6.3513e-02,  6.3862e-02, -1.5441e-01,\n",
       "                        -3.3501e-02, -2.4070e-01, -1.9097e-01],\n",
       "                       [ 4.4322e-02, -2.6251e-01,  1.1973e-01, -2.5137e-01, -6.7071e-01,\n",
       "                        -3.6944e-01,  1.3779e-01, -2.7227e-02,  6.9558e-02,  6.1839e-02,\n",
       "                         1.2696e-01, -7.1387e-01, -6.1842e-01,  8.1697e-02, -5.4608e-01,\n",
       "                        -2.0451e-01, -3.1318e-01, -3.4733e-01, -1.2807e-01, -2.6323e-01,\n",
       "                        -3.6847e-01, -6.9091e-01, -3.6975e-02, -8.4762e-01,  5.1659e-02,\n",
       "                        -4.1577e-01,  5.4473e-02, -1.5061e-01,  9.8946e-02, -7.3106e-01,\n",
       "                         1.6465e-01, -3.4602e-01, -3.6834e-01,  4.6344e-02,  1.3479e-01,\n",
       "                         9.3781e-02, -4.2380e-01,  7.2575e-02, -1.7805e-01, -6.3213e-02,\n",
       "                        -8.9896e-01, -3.2754e-01, -6.3098e-02, -1.0018e-01, -4.2445e-02,\n",
       "                         4.8019e-03, -7.5056e-01,  6.0299e-02, -6.4407e-01, -4.1281e-01,\n",
       "                        -1.7766e-01, -4.0416e-01, -8.0473e-01,  8.1331e-02, -3.6939e-01,\n",
       "                        -3.8163e-01, -2.1953e-01,  7.7147e-02,  5.7171e-02, -1.0949e-02,\n",
       "                         8.2327e-02, -2.6818e-01,  1.1218e-01, -1.4522e-01,  5.1491e-02,\n",
       "                        -9.0205e-02, -7.4493e-01, -1.9258e-01, -1.3984e-01, -3.1711e-01,\n",
       "                        -5.0031e-01, -2.1867e-02, -4.8001e-01, -6.3641e-01, -6.5373e-01,\n",
       "                         4.8865e-02, -7.5491e-02, -3.0565e-01, -2.7780e-01, -5.4826e-01,\n",
       "                         1.4033e-01, -1.1707e+00, -1.3613e-01, -2.3528e-01, -9.8549e-01,\n",
       "                         1.3835e-01, -9.3348e-02, -6.9579e-01, -1.2322e-02, -3.9108e-01,\n",
       "                        -1.5261e+00, -2.0880e-01, -2.1027e-01, -4.8048e-01,  2.2296e-03,\n",
       "                        -6.9192e-01, -1.0822e-01, -6.8506e-01],\n",
       "                       [-9.3650e-02, -7.8182e-02,  1.6746e-01, -5.1328e-01, -2.5913e-01,\n",
       "                        -1.2394e-01,  3.3046e-03,  1.5678e-02, -3.1828e-01, -3.8878e-02,\n",
       "                        -1.2032e-01, -1.0228e+00, -2.2920e-01, -2.4052e-01, -3.7711e-01,\n",
       "                        -2.2572e-01, -5.8813e-01,  4.4079e-02,  8.8769e-03, -2.2573e-01,\n",
       "                        -8.8563e-02, -9.5968e-01, -1.6330e-01, -8.5558e-02,  1.2996e-01,\n",
       "                        -1.2464e-01, -9.0285e-02, -8.7719e-01, -8.6169e-02, -2.4423e-01,\n",
       "                        -3.7963e-02, -9.7258e-01, -1.0802e-01, -7.6795e-01,  5.0003e-02,\n",
       "                        -8.0017e-02, -1.4118e+00, -8.6950e-02, -1.9223e-01, -4.0354e-02,\n",
       "                         1.0487e-01, -9.5239e-01, -6.1048e-02, -2.2344e-01, -3.6116e-01,\n",
       "                         8.9745e-02, -2.2477e-01, -1.2811e-01, -1.2403e-01, -6.3335e-01,\n",
       "                         1.7138e-01, -2.0539e-01, -4.1232e-01, -1.1157e-01, -7.1585e-01,\n",
       "                        -1.2909e-01, -2.6255e-01, -1.9110e-01,  6.1177e-02,  1.2728e-01,\n",
       "                        -2.4279e-02, -9.8455e-02, -3.6677e-01,  8.4629e-02,  1.8920e-02,\n",
       "                        -5.3896e-01, -3.3532e-01, -7.4264e-01, -2.1083e-01, -6.8004e-01,\n",
       "                        -4.1480e-01, -4.3680e-01, -2.4679e-01, -3.1155e-02, -7.0648e-01,\n",
       "                         1.0012e-01, -1.2141e-01, -3.3740e-01, -2.9262e-01, -1.3173e-01,\n",
       "                        -9.0065e-02,  9.7317e-02, -9.1439e-02, -1.2286e+00, -3.1423e-02,\n",
       "                        -9.5043e-02, -3.8439e-01, -7.6019e-01, -2.7176e-01, -1.5717e-01,\n",
       "                        -3.0722e-01, -6.0240e-04, -1.0484e-01, -1.8567e-01, -1.6258e-01,\n",
       "                        -8.8318e-02, -2.7314e-01, -6.0577e-01],\n",
       "                       [-1.6300e-01, -3.6294e-01,  8.9986e-02, -3.6023e-01, -3.6626e-01,\n",
       "                        -5.4134e-01,  1.0226e-01,  9.0529e-02,  1.4383e-01,  5.8084e-02,\n",
       "                         8.9960e-02, -6.4538e-01, -7.6297e-01,  1.7699e-01, -4.1569e-01,\n",
       "                        -4.8092e-01, -2.3735e-01, -2.9837e-01, -1.4851e-01, -4.7514e-01,\n",
       "                        -1.0308e+00, -6.9044e-01, -8.0262e-03, -3.4620e-01,  4.7659e-02,\n",
       "                        -6.7840e-01,  6.4724e-02, -4.2748e-01,  7.8682e-02, -4.1671e-01,\n",
       "                         4.3712e-02, -2.6536e-01, -1.7028e-01, -1.2953e-01,  1.9866e-01,\n",
       "                        -1.6395e-02, -1.0954e+00,  5.7162e-02, -1.5948e-01, -1.1624e-01,\n",
       "                        -4.8549e-01, -6.2810e-01, -7.9036e-02, -1.5076e-01, -2.0655e-01,\n",
       "                        -4.5365e-02, -5.4219e-01, -7.4769e-02, -4.6832e-01, -8.9183e-01,\n",
       "                        -1.4947e-02, -2.0346e-01, -5.9781e-01,  2.5661e-02, -1.2298e+00,\n",
       "                        -2.3181e-01, -1.7309e-01,  1.6108e-01, -1.0631e-01, -9.5859e-02,\n",
       "                        -4.3472e-02, -3.5835e-01,  1.9752e-01, -3.0542e-01,  2.3873e-02,\n",
       "                        -2.5896e-01, -3.7343e-01,  8.8047e-02, -2.4699e-01, -4.5823e-01,\n",
       "                        -8.6308e-01, -7.3993e-02, -6.1307e-01, -8.1291e-01, -5.7573e-01,\n",
       "                         9.6744e-03, -4.5415e-01, -4.7822e-01, -7.0348e-02, -1.9904e-01,\n",
       "                         3.9884e-02, -5.2157e-01, -2.5398e-01, -4.4461e-01, -3.1985e-01,\n",
       "                         7.4058e-02, -3.0087e-01, -1.7025e+00, -3.5473e-01, -2.6142e-01,\n",
       "                        -1.2369e+00, -1.1328e-01, -2.4021e-01, -6.4845e-01, -2.3720e-01,\n",
       "                        -2.1335e-01, -1.8430e-01, -1.0569e+00]])),\n",
       "              ('o.bias',\n",
       "               tensor([-0.1808, -2.1373,  0.3520,  0.2080,  0.2256, -0.5331,  0.3357,  0.1825,\n",
       "                        0.6976, -0.7297]))])}"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note this model needs to be loaded into a model with the same architecture as the old model\n",
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(checkpoint['input'],checkpoint['hidden'][0])\n",
    "        self.h2 = nn.Linear(checkpoint['hidden'][0],checkpoint['hidden'][1])\n",
    "        self.h3 = nn.Linear(checkpoint['hidden'][1],checkpoint['hidden'][2])\n",
    "        self.o = nn.Linear(checkpoint['hidden'][2],checkpoint['output'])\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "    def forward(self,x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.dropout(F.relu(self.h1(x)))\n",
    "        x = self.dropout(F.relu(self.h2(x)))\n",
    "        x = self.dropout(F.relu(self.h3(x)))\n",
    "        x = F.log_softmax(self.o(x),dim=1)\n",
    "        return x\n",
    "new_model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8555931448936462\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "with torch.no_grad():\n",
    "    new_model.eval()\n",
    "    for images,labels in testloader:\n",
    "        log_ps = new_model(images)\n",
    "        ps = torch.exp(log_ps)\n",
    "        top_p,top_class = ps.topk(1,dim=1)\n",
    "        matches = (top_class == labels.view(*top_class.shape)).type(torch.FloatTensor)\n",
    "        accuracy += matches.mean()\n",
    "    print(f'Accuracy: {accuracy/len(testloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Up!\n",
    "\n",
    "In the next part, I'll show you how to save your trained models. In general, you won't want to train a model everytime you need it. Instead, you'll train once, save it, then load the model when you want to train more or use if for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3],\n",
       "        [5],\n",
       "        [3],\n",
       "        [8],\n",
       "        [8],\n",
       "        [4],\n",
       "        [3],\n",
       "        [9],\n",
       "        [8],\n",
       "        [3],\n",
       "        [5],\n",
       "        [1],\n",
       "        [6],\n",
       "        [6],\n",
       "        [7],\n",
       "        [0]])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
